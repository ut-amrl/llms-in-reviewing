Title: Proving Test Set Contamination in Black-Box Language Models

Summary:
The paper addresses the problem of detecting whether a language model has been trained on, or “contaminated” by, a given benchmark dataset, a concern that impacts the validity of reported performance gains on standard benchmarks. The authors introduce a permutation-based statistical test to diagnose contamination without access to the model’s training data or internal weights. By leveraging the property of exchangeability (where the order of examples in a truly held-out test set should not matter), they show that a contaminated model will exhibit systematically higher probabilities for the original “canonical” ordering of the test data than for random permutations. Their proposed sharded log-likelihood comparison test provides provable false-positive guarantees and displays high sensitivity in experiments—even when the target dataset appears only a few times in the training corpus. They also apply their method to a few well-known public language models and find limited evidence of large-scale contamination.

Strengths:
1. Novel Statistical Framework: The paper provides a formal statistical test (permutation-based, leveraging exchangeability) that turns the suspicion of data contamination into a hypothesis-testing problem with theoretical guarantees. This is a clear methodological contribution to the field of LLM evaluation.  
2. Practical and Black-Box: The test only requires log-likelihood queries, making it directly applicable to commercial APIs or closed-source models where accessing training data or model weights is impossible.  
3. Provable Guarantees: They carefully demonstrate that their permutation tests offer bounded false-positive rates, ensuring that flagged contamination events cannot be trivially dismissed as random variance or erroneous.  
4. Empirical Rigor: Through canary experiments, the authors train a 1.4B-parameter language model on known contaminated data and show that the proposed test reliably detects contamination at duplication levels as low as 4× or 7× for small benchmarks. The approach exhibits strong detection power when duplication rates are higher (≥10×).  
5. Public Benchmark Release: They release their contaminated model and data splits to encourage further research. This transparency adds to reproducibility and fosters community-wide improvements on contamination detection.

Weaknesses:
1. Single-Duplication Limits: While the method works as soon as duplication rates reach around 2× or 4×, the results show it struggles to detect single duplications. This may limit the test’s applicability in edge cases where a dataset is accidentally included once or only a very small number of times.  
2. Limited Real-World Benchmarks: The paper’s final experiments cover several popular benchmarks and do not generally find strong contamination signals. While this is reassuring, it would be helpful to expand the scope to new or larger-scale benchmarks that might appear in real training corpora.  
3. Dataset Exchangeability Requirements: The theoretical guarantees presume i.i.d. or exchangeable data. In practice, many benchmarks (e.g., reading comprehension tasks derived from sequential documents) may contain structured sequences or repeated examples that break strict exchangeability. The authors address this by filtering out obviously sequential or repeated examples, but this might complicate real-world usage.  
4. Limited Comparisons to Existing Heuristics: Although the authors reference related work on membership inference and n-gram overlap checks, more direct, side-by-side comparisons—especially with other black-box contamination detection approaches—would strengthen the experimental section.  
5. Multiple Testing Procedures: The authors mention controlling false-positive rates but do not conduct rigorous multiple comparison corrections for their broad sets of real-world benchmarks. More detail on controlling the family-wise error rate or false discovery rate across numerous tests would improve the paper’s completeness.

Suggestions for Improvement:
• Investigate detection in the single-duplication regime in more depth. If it is provably infeasible, clarifying that with theoretical or empirical justification would be valuable.  
• Expand the number and variety of real-world benchmark datasets tested, possibly including more recent or specialized benchmarks.  
• Provide further details on how to handle structured or partially exchangeable datasets in practice—for instance, guidelines on how to prune or process data that includes large amounts of repeated content.  
• Include more direct empirical comparisons against other contamination or membership inference methods, especially those that do not rely on exchangeability, to highlight the advantages/disadvantages of the proposed approach.  
• Offer a thorough multiple hypothesis testing correction or at least a discussion on best practices for controlling false positives across many simultaneously tested benchmarks.

Overall Recommendation: Accept

Rationale:  
The paper makes a strong methodological contribution to detecting test-set contamination in black-box language models, a key challenge in accurately assessing modern LLMs. The statistical grounding, black-box usability, and demonstrated empirical efficacy for moderate duplication rates are compelling. Despite some limitations—particularly for single-duplication detection and real-world intricacies of exchangeability—this work represents an important step forward in reliable LLM auditing. Its novel approach and robust experimental validation merit acceptance at a top-tier conference.