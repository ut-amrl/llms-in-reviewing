Title: “Proving Test Set Contamination in Black-Box Language Models”

Summary:
The paper proposes a novel statistical test to detect “test set contamination” in large language models (LLMs) without requiring direct access to the pretraining dataset or the model’s weights. The authors leverage the assumption of exchangeability of many standardized benchmarks, noting that if a model has memorized a dataset in its original (canonical) order, it will assign notably higher likelihood to that sequence than to randomly permuted versions. They design a “sharded likelihood comparison” approach to improve efficiency and statistical power over a simple permutation test. Experiments on a 1.4B-parameter language model (trained from scratch with known forms of contamination) show the method’s ability to detect test set duplication at relatively low duplication rates (around four or more). Finally, they audit four publicly available LLMs and generally find no evidence of severe test set contamination, except for a small signal on certain benchmarks.

Strengths:
1. Novel Contamination Detection Method:  
   • The exchangeability-based approach is elegant, exploiting a fundamental statistical property to achieve provable false-positive guarantees.  
   • The “sharded” permutation method is computationally more tractable than a direct global permutation test while being effective for detecting contamination signals.

2. Rigorous Theoretical Underpinnings:  
   • The paper provides formal statistical arguments and asymptotic guarantees for the proposed test.  
   • The authors carefully distinguish contamination as a dependence relationship between the model and a target dataset, emphasizing the significance of i.i.d. assumptions and how it yields exchangeability-based results.

3. Well-Executed Experiments:  
   • Training a new 1.4B-parameter model with carefully inserted “canary” datasets demonstrates empirical efficacy.  
   • The test succeeds under various duplication rates (from 1 to 100, across multiple standard benchmarks such as BoolQ, MNLI, MMLU).  
   • The paper also includes audits of popular models (LLaMA2, Mistral, Pythia, GPT-2 XL), showcasing how third-party testers can apply the method in practice.

4. Potential Practical Utility:  
   • Results suggest that the approach generalizes well to real-world black-box LLM scenarios with minimal overhead, allowing for routine external audits.  
   • The paper highlights an open-source benchmark for future method comparisons, which fosters reproducibility.

Weaknesses:
1. Single-Duplication Sensitivity:  
   • Although the method shows strong detection above a certain duplication threshold, it generally fails reliably at duplication counts of one (a more subtle form of contamination). This is acknowledged, but it remains a key limitation in fully guaranteeing that a large model has never “glanced” at a small benchmark.

2. Reliance on Dataset Exchangeability:  
   • The authors acknowledge that many public benchmarks have partial ordering or other non-exchangeable quirks (e.g., repeated or indexed examples). This complicates usage and may generate spurious false positives or require manual filtering.

3. Limited Detection Scope:  
   • The current method identifies primarily “verbatim” or “near-verbatim” memorization-based contamination. It does not address more subtle contamination scenarios (e.g., when the training data includes a source that partially overlaps with benchmark content without including exact records).

4. Dependent on Model Queries:  
   • While the black-box nature is valuable, extensive log-likelihood queries can become expensive for very large datasets or for models with restrictive API constraints. Further optimizations or alternative query strategies may be needed in large-scale practical audits.

Suggestions for Improvement:
1. Expand on Strategies to Address Single-Duplication Cases:  
   • Investigate refined statistics or ensemble approaches to handle borderline contamination (e.g., duplication rate = 1). Future work might focus on distinguishing partial memorization from the baseline distribution more sensitively.

2. Provide Automated Checks for (Non)-Exchangeability:  
   • A consistent pipeline for detecting non-exchangeable elements in a benchmark (removing duplicates, indexes, or special ordering cues) could further strengthen the reliability of the test and mitigate false positives.

3. Investigate More Complex Forms of Contamination:  
   • Showing how or whether the approach can be extended to partial contamination (e.g., partially paraphrased content) would broaden its impact.  
   • Combining this exchangeability-based test with advanced membership inference techniques might yield greater coverage.

4. Offer More Detailed Cost Analysis:  
   • While the sharded approach is more efficient than direct permutation tests, a breakdown of computational costs (especially for large or real-world benchmarks) would be helpful to gauge feasibility.

Overall Recommendation:  
Accept (with Enthusiasm)

Rationale:  
This paper tackles an increasingly crucial issue—unintentional or intentional inclusion of public benchmarks in the training data of LLMs—and does so with a novel, theoretically grounded test that offers provable guarantees. The experiments are carefully crafted, and the authors propose a framework that could be widely adopted by the community for third-party audits. While there are some natural limitations (e.g., single-duplication detection) and practical constraints (e.g., the assumption of exchangeability), overall, the paper signifies a notable step forward toward transparent and reliable benchmark evaluations. The soundness of the theoretical framework, combined with the compelling empirical evidence, suggests it merits acceptance at a top-tier conference.