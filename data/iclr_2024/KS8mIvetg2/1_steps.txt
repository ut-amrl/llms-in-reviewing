system: You are an expert reviewer for AI conference papers.
user: Read the following paper, in preparation to review it. Maintain healthy scientific skepticism, don't assume that the claims in the paper are necessarily correct, and check everything carefully:
PROVING TESTSETCONTAMINATION IN
BLACK BOXLANGUAGE MODELS
Yonatan Oren1∗, Nicole Meister1∗, Niladri Chatterji1∗, Faisal Ladhak2, Tatsunori B. Hashimoto1
1Stanford University,2Columbia University
yonatano@cs.stanford.edu
{nmeist, niladric, thashim }@stanford.edu
faisal@cs.columbia.edu
ABSTRACT
Large language models are trained on vast amounts of internet data, prompting
concerns and speculation that they have memorized public benchmarks. Going
from speculation to proof of contamination is challenging, as the pretraining data
used by proprietary models are often not publicly accessible. We show that it is
possible to provide provable guarantees of test set contamination in language mod-
els without access to pretraining data or model weights. Our approach leverages
the fact that when there is no data contamination, all orderings of an exchange-
able benchmark should be equally likely. In contrast, the tendency for language
models to memorize example order means that a contaminated language model
will find certain canonical orderings to be much more likely than others. Our
test flags potential contamination whenever the likelihood of a canonically or-
dered benchmark dataset is significantly higher than the likelihood after shuffling
the examples. We demonstrate that our procedure is sensitive enough to reliably
prove test set contamination in challenging situations, including models as small
as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets
that appear only a few times in the pretraining corpus. Using our test, we audit
four popular publicly accessible language models for test set contamination and
find little evidence for pervasive contamination.
1 I NTRODUCTION
Large language models (LLMs) have driven remarkable improvements on a number of natural lan-
guage processing benchmarks (Wang et al., 2019) and professional exams (OpenAI, 2023). These
gains are driven by large-scale pretraining on massive datasets collected from the internet. While
this paradigm is powerful, the minimal curation involved has led to growing concerns of dataset
contamination, where the pretraining dataset contains various evaluation benchmarks. This con-
tamination leads to difficulties in understanding the true performance of language models – such as
whether they simply memorize the answers to difficult exam questions. Disentangling the effects
of generalization and test set memorization is critical to our understanding of language model per-
formance, but this is becoming increasingly difficult as the pretraining datasets are rarely public for
many of the LMs deployed today.
Although there is ongoing work by LLM providers to remove benchmarks from pre-training datasets
and perform dataset contamination studies, such filtering can fail due to bugs (Brown et al., 2020a),
be limited to a select set of benchmarks (Brown et al., 2020a; Wei et al., 2021; Chowdhery et al.,
2022), and requires trust in these vendors. Increasing competitive pressures have also led to some
recent model releases to include no contamination studies at all (OpenAI, 2023). These factors make
it critical for us to be able to audit existing language models for the presence of benchmark datasets
without the cooperation of language model providers.
In parallel to contamination studies, there has been a growing literature on heuristic membership
inference algorithms, that seek to reverse engineer aspects of the pretraining dataset (Carlini et al.,
*Equal technical contribution, first author was the project lead.
1Figure 1: Given a pre-training dataset contaminated with the BoolQ(Clark et al., 2019) test set
(left), we detect such contamination by testing for exchangability of the dataset (right). If a model
has seen a benchmark dataset, it will have a preference for the canonical order (i.e. the order that
examples are given in public repositories) over randomly shuffled examples orderings. We test for
these differences in log probabilities, and aggregate them across the dataset to provide false positive
rate guarantees.
2019; Mattern et al., 2023) as well as provide some evidence for test set contamination (Sainz et al.,
2023; Golchin & Surdeanu, 2023). However, the heuristic nature of these methods limits their
usefulness, as these methods cannot elevate speculation about a suspected instance of test set con-
tamination into an irrefutable proof of contamination.
In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test
set contamination in black box language models. More specifically, we provide a statistical test
that can identify the presence of a benchmark in the pre-training dataset of a language model with
provable false positive rate guarantees and without access to the model’s training data or weights.
To achieve these guarantees, we exploit the fact that many datasets have a property known as ex-
changeability , where the order of examples in the dataset can be shuffled without affecting its joint
distribution. Our key insight is that if a language model shows a preference for any particular or-
dering of the dataset – such as a canonical ordering that appears in publicly available repositories –
this violates exchangeability and can only occur by observing the dataset during training (Figure 1).
We leverage this insight to propose a set of tests that compares the language model’s log probabil-
ity on the ‘canonical’ ordering (taken from public repositories) to the log probability on a dataset
with shuffled examples, and flag a dataset if the two log probabilities have statistically significant
differences.
Using these ideas, we propose a computationally efficient and statistically powerful test for con-
tamination which shards the dataset into smaller segments and performs a series of log probability
comparisons within each shard. We prove that this sharded test provides control over the false pos-
itive rate, enables computationally efficient parallel tests, and substantially improves the power of
the test for small p-values.
We evaluate our statistical test on a 1.4 billion parameter language model trained on a combination
of Wikipedia and a curated set of canary test sets. Our test is sensitive enough to identify test sets
with as few as 1000 examples, and sometimes even appearing only twice in the pretraining corpus.
In the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain
vanishingly small p-values on our test. Finally, we run our test on four commonly used, public
language models to study the behavior of our test on language models in the wild and find little
evidence of pervasive and strong test set contamination.
We summarize our contributions below.
• Demonstrating the use of exchangability as a way to provably identify test set contamina-
tion using only log probability queries.
• Construction of an efficient and powerful sharded hypothesis test for test set contamination.
• Empirical demonstration of black-box detection of contamination for small datasets that
appear few times during pretraining.
2Our three contributions suggest that black-box identification of test set contamination is practical
and further improvements in the power of the tests may allow us to regularly audit language models
in the wild for test set contamination. To encourage the development of new provable guarantees
for test set contamination, we release our pretrained models as a benchmark for developing future
statistical tests.1.
2 P ROBLEM SETTING
Our high-level goal is to identify whether the training process of a language model θincluded dataset
X. In our setting, the only method we have to study θis through a log probability query logpθ(s)for
a sequence s(i.e. no access to dataset or parameters). This setting mirrors many common situations
with API-based model providers (Brown et al., 2020b; Bai et al., 2022) and matches an increasing
trend where the training data is kept secret for ‘open’ models (Touvron et al., 2023; Li et al., 2019).
Provably identifying test set contamination can be viewed as a hypothesis test in which the goal is
to distinguish between two hypotheses:
•H0:θis independent of X
•H1:θis dependent on X
where we treat θas a random variable whose randomness arises from a combination of the draw
of the pretraining dataset (potentially including X) and we will propose a hypothesis test with the
property that it falsely rejects the null hypothesis H0with probability at most α.
False positives under H0In most cases, we can make use of a property of a dataset known as
exchangeability to obtain our false positive guarantee. Nearly all datasets can be expressed as a
collection of examples X:={x1. . . x n}where the ordering of the examples are unimportant, and
the probability of any ordering would be equally likely (i.e. p(x1. . . x n) =p(xπ1. . . x πn)for any
permutation π). Notably, this assumption would hold under the standard assumption that the dataset
is a collection of i.i.d examples.
Whenever exchangability of the dataset holds, the log probabilities of the model under H0must have
a useful invariance property,
Proposition 1. Let seq (X)be a function that takes a dataset Xand concatenates the examples to
produce a sequence, and let Xπbe a random permutation of the examples of Xwhere πis drawn
uniformly from the permutation group. For an exchangeable dataset Xand under H0,
logpθ(seq(X))d= log pθ(seq(Xπ)).
Proof This follows directly from the definitions of exchangability and H0. Since Xis ex-
changable, seq (X)d=seq(Xπ)and by the independence of θfrom Xunder H0, we know that
(θ,seq(X))d= (θ,seq(Xπ)). Thus, the pushforward under logpθ(seq(X))must have the same
invariance property.
Proposition 1 is the basic building block of our tests. It implies that the log probabilities of X
under H0have the same distribution when shuffled, and this permutation invariance will enable us
to directly apply standard results on constructing permutation tests (Lehmann & Romano, 2005).
Detection rate under H1The false positive rate guarantee holds with extremely weak assump-
tions, but a useful test should also have high power, meaning that it should have a high detection
rate under H1. We cannot hope for high detection rate without further assumptions. For instance,
an adversary may hide an encrypted copy of Xwithin the parameters of the model (which would
induce a clear dependence between the model and X) but it would be nearly impossible for us to
detect such a situation even with weight access.
1https://github.com/tatsu-lab/test_set_contamination
3However, most existing forms of contamination are benign . In the benign contamination setting we
consider, pretraining datasets become contaminated when test sets accidentally slip through filtering
mechanisms Brown et al. (2020a). In this case, we have a reasonable expectation that the invariance
in proposition 1 will be violated and logpθ(seq(X))≫logpθ(seq(Xπ))as the language model θis
explicitly trained to maximize the log-likelihood over its training data, including seq (X). The vio-
lation of exchangability allows us to reliably detect test set contamination, and the existing literature
on memorization (Carlini et al., 2021) suggests that many models may verbatim memorize the order
of examples in a benchmark dataset. We now focus on building tests that can reliably identify this
form of memorization.
3 M ETHODS
The core idea of our statistical test is to compare the log probability of the dataset under its original
ordering to the log probability under random permutations. We begin by describing the basic version
of this idea, which directly implements a permutation test on the log probabilities. We then identify
some drawbacks of this approach and describe a sharded test which improves the statistical power
and computational efficiency of the test.
3.1 A PERMUTATION TEST FOR CONTAMINATION
Under the null hypothesis, the likelihood under the model of any permutation of the dataset Xπhas
the same distribution, and thus the rank of logpθ(seq(X))among any set of randomly permuted
probabilities {logpθ(seq(Xπ1)). . .logpθ(seq(Xπn))}will be a uniform random variable over [n+
1](Lehmann & Romano, 2005, Theorem 15.2.2).
This can be used directly to construct a permutation test. Consider the proportion of permuted copies
ofXwith lower log-likeihood than the canonical ordering under the model,
p:=E[ 1{logpθ(seq(X))<logpθ(seq(Xπ))}].
The distribution of pwill be uniform under H0, and we can test for contamination at a significance
levelαby rejecting H0when p < α . In practice, computing this expectation over all πis intractable,
and we replace this with a Monte Carlo estimate and the appropriate finite-sample correction (Phip-
son & Smyth, 2010), which gives
ˆp:=Pm
i=11{logpθ(seq(X))<logpθ(seq(Xπm))}+ 1
m+ 1.
This test is simple and straightforward to implement, and the validity of this test when rejecting at
ˆp≤αis clear from standard results on permutation testing (Lehmann & Romano, 2005; Phipson &
Smyth, 2010). However, this test suffers from a major drawback in its Monte Carlo implementation
– the runtime of the test in terms of the number of log probability computations is O(m|X|)for a
sequence of length |X|and the p-value can never be below 1/(m+ 1). For hypothesis tests that aim
to reject at very low p-values (or with substantial multiple hypothesis testing corrections), this poses
a tradeoff between statistical power and computational requirements.
3.2 A SHARDED LIKELIHOOD COMPARISON TEST FOR CONTAMINATION
What are some drawbacks of the naive permutation test? It has an undesirable tradeoff between
statistical power and computational requirements for small α, and also requires that the model assign
higher likelihood to the canonical ordering Xthan nearly allshuffled orderings of Xπ. This latter
condition can also be a serious problem, as the model may have biases the prefer certain orderings
(e.g. ones that place duplicate examples next to each other) regardless of the order seen during
training.
A more likely assumption would be that the log-probability under the canonical ordering Xis higher
than the average log probability under a random permutation. That is, instead of relying on the
quantile E[ 1{logpθ(seq(X))<logpθ(seq(Xπ))}], can we instead perform multiple log probability
comparisons of the form logpθ(seq(X))<E[logpθ(seq(Xπ))]?
4We show that this is possible and the resulting test resembles a series of log probability comparisons
followed by a t-test to aggregate these results. More specifically, we will partition the examples
X1,···, Xnintorcontiguous shards S1. . . S rformed by grouping together adjacent examples
S1= (X1, X2,···, Xk)
where each shard Sicontains at least k=n/rexamples.
Then, we will permute the examples within each shard and compare the likelihood of the canonical
ordering to a Monte Carlo estimate of the average likelihood of the shuffled ordering as
si:= log pθ(seq(X))−Mean π(logpθ(seq(Xπ))).
Finally, to construct the test, we aggregate these shard statistics sivia the mean s=1
rPr
i=1siand
test for whether sis zero-mean using a t-test.
This statistical test, whose pseudocode is given in Algorithm 1, addresses the shortcoming of the
permutation test by converting a single rank comparison into a collection of log probability com-
parisons. The t-test based approach also requires O(m|X|)runtime for mpermutations, but there
is no 1/mminimum p-value, and in practice we find that the p-values obtained by this approach
decay rapidly, as it only requires that the language models consistently assign higher-than-average
log probabilities to the canonical ordering, rather than requiring that the canonical log probability
be in the tails of the permutation null distribution.
Algorithm 1 Sharded Rank Comparison Test
Require: Test set examples x1, . . . , x n
Require: Target model θ
Require: Number of shards r
Require: Number of permutations per shard m
1:Partition the examples into shards S1, S2,···, Sr, where each shard has at least ⌊n/r⌋exam-
ples, and one extra example is added to the first nmod rshards.
2:foreach shard Sido
3: Compute the log-likelihood of the canonical order:
l(i)
canonical := log pθ(seq(x(i)
1, x(i)
2,···, x(i)
k))
4: Estimate l(i)
shuffled :=Mean π[logpθ(seq(x(i)
π(1),···, x(i)
π(k)))]by computing the sample average
overmrandom permutations π.
5: Compute si=l(i)
canonical −l(i)
shuffled
6:end for
7:Define s=1
rPr
i=1sithe sample average over the shards.
8:Run a one-sided t-test for E[si]>0, returning the associated p-value of the test as p.
Under the null, we expect sto be the sum of independent random variables and we can now show
that the overall test provides a false positive rate guarantee.
Theorem 2. Under the null hypothesis, an i.i.d dataset X, and finite second moments on logθ(S),
|P(p < α )−α| →0
asm→ ∞ andpis defined as the p-value in Algorithm 1.
Proof The result follows directly from the combination of Theorem 1 and standard invari-
ance results in (Lehmann & Romano, 2005). First, by Theorem 1, note that the distribution of
logpθ(seq(x(i)
π(1),···, x(i)
π(k))is invariant to the permutation π.
By Lehmann & Romano (2005, Theorem 15.2.2), this guarantees that the permutation distribution
is uniform over the support, and the statistic simust be zero-mean. Next, we note that each
shard is independent, as each example is split independently into a separate shard with no overlap.
By independence and the finite second moment condition, s→N(0, σ2/√m)under the null
by the central limit theorem and a one sided t-test provides asymptotically valid p-values with
5P(p < α )→αuniformly as m→ ∞ (Lehmann & Romano, 2005, Theorem 11.4.5).
This result ensures that the sharded rank comparison test also provides (asymptotic) guarantees on
false positive rates, much like the permutation test. The test we propose here has two small dif-
ferences relative to the permutation test – it provides asymptotic, rather than finite-sample valid
p-values and assumes i.i.d Xfor the proof. These conditions could be relaxed by the use of Berry-
Esseen bounds to obtain finite-sample convergence rates for the CLT as well as replacing our use
of a standard central limit theorem with one applicable to the sums of exchangable random vari-
ables. However, we opted to present the simpler asymptotic test given the frequent use of i.i.d data
generation assumption in the literature as well as the fast convergence of the CLT in practice.
4 E XPERIMENTS
We now demonstrate that our test is effective for detecting many common forms of test set contam-
ination. We begin by training a 1.4 billion parameter language model, consisting of both Wikipedia
and a known collection of exchangeable test sets. These canaries serve as positive controls for our
test, and our goal will be to flag as many of these as possible. Having validated the test in a setting
with known contamination, we then explore its use with existing open models.
4.1 P RETRAINING WITH INTENTIONAL CONTAMINATION
Datasets and training To validate our test statistic, we train a 1.4 billion parameter GPT-2 model
from scratch with a combination of standard pretraining data (Wikitext, taken from the RedPajama
corpus (Together Computer, 2023)) and known test sets. We derive 10 test sets from numerous
standard datasets (BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OpenbookQA (Mi-
haylov et al., 2018b), MNLI (Williams et al., 2018), Natural Questions (Kwiatkowski et al., 2019a),
TruthfulQA (Lin et al., 2022), PIQA (Bisk et al., 2019), MMLU (Hendrycks et al., 2021)), and sub-
sample the datasets to around 1000 examples to ensure that the test sets remain a small part of the
overall pretraining dataset (See Table 1 for exact sizes). While we do not know if these datasets
are exchangable when they were constructed, we can make them exchangable simply by applying a
random shuffle to the dataset, which would make all orderings of the examples equally likely.
To test our ability to detect benchmarks at various duplication rates, we duplicate each of the datasets
a different number of times - ranging from 1 to 100 (See Table 1). The overall pretraining dataset
has 20.2B tokens, with 20M tokens associated with some benchmark dataset.
Test parameters The sharded rank comparison test requires two additional parameters: the shard
count mand the permutation count r. Thoughout these experiments we use m= 50 shards and
r= 51 permutations. In our ablations below, we found that the tests are not particularly sensitive to
these parameters, and we fix these parameters to avoid the possibility of p-hacking.
Canary Results In Table 1, we find that our test is highly sensitive, and provides near-zero p-
values at duplication rates of 10 or above. These detections hold for relatively small datasets ( ≤1000
examples) and for a modestly sized language model with 1.4 billion parameters. Given that many
test sets are much larger in practice, and many language models of interest are much larger and
memorize more aggressively (Carlini et al., 2019), these findings suggest that our test is likely to
detect contamination in practice.
While the permutation test attains significance (at a typical α= 0.05, say) for all benchmarks
duplicated at least 10 times, the p-values are bounded below by 1/(1 + r), where the number of
permutations rused here is 100. Results for our sharded test use r= 50 ; even with half the
compute, the sharded test attains comparable performance for benchmarks with small duplication
rate. However, the p-values attained by the sharded test for moderate to high duplication rates are
vanishingly small.
Attaining comparably low p-values using the permutation test is computationally infeasible. For
example, to allow for the possibility of a p-value as low as 1.96e-11 (matching the MNLI result)
would require permuting the dataset 1011times, and as many forward passes of the model.
6Table 1: We report the results of training a 1.4B language model from scratch on Wikitext with
intentional contamination. For each injected dataset, we report the number of examples used (size),
how often the test set was injected into the pre-training data (dup count), and the p-value from the
permutation test and sharded likelihood comparison test. The bolded p-values are below 0.05and
demonstrate in the case of higher duplication counts, such as datasets appearing 10 or more times,
we obtain vanishingly small p-values on our test. Finally, rows marked 1e-38 were returned as
numerically zero due to the precision of our floating point computation.
Name Size Dup Count Permutation p Sharded p
BoolQ 1000 1 0.099 0.156
HellaSwag 1000 1 0.485 0.478
OpenbookQA 500 1 0.544 0.462
MNLI 1000 10 0.009 1.96e-11
TruthfulQA 1000 10 0.009 3.43e-13
Natural Questions 1000 10 0.009 1e-38
PIQA 1000 50 0.009 1e-38
MMLU Pro. Psychology 611 50 0.009 1e-38
MMLU Pro. Law 1533 50 0.009 1e-38
MMLU H.S. Psychology 544 100 0.009 1e-38
Although our test is unable to detect contamination at a duplication rate of 1, other existing literature
on memorization has suggested that detection at this duplication level is extremely difficult. Prior
work has found that existing tests of memorization begin to work with 10-30 duplicates (Carlini
et al., 2021), that deduplicated text is hard to extract (Kandpal et al., 2022), and that dataset con-
tamination with a duplication rate of 1 barely affects downstream benchmark performance (Magar
& Schwartz, 2022).
Power as a function of duplication rate. We carefully study the lowest duplication rate for which
our test can reliably detect contamination. To do this, we perform the above canary study but with
duplication rates ranging from 1 to 7, and we show the aggregate log p-values for each duplication
rate in Figure 2. We find that we cannot reliably detect duplication rates of 1, but that at counts of
2 and 4 we begin to detect some test sets (gray points below the dashed line) and that the detection
threshold is around a duplication rate of 4. This suggests that even small amounts of dataset du-
plication would be sufficient for detection, and future improvements to the power of this test could
enable reliable detection at much lower duplication rates.
Figure 2: For a model pre-trained with canary datasets injected at a duplication count of 1, 2, 4, and
7, we plot the log p-value against dataset duplication count to quantify how the test’s power depends
on dataset duplication count.
A public benchmark of provable test set contamination Our work demonstrates that exploiting
exchangability allows us to provably detect test set contamination even for small, low-duplication
count datasets. However, it is an open question whether there are tests that can reliably detect
contamination at a duplication rate of 1. To support future work on this open problem, we release
7our pre-trained models trained on Wikitext mixtures together with the corresponding canary test
sets2.
In addition to the release, we will maintain a leaderboard of methods that provide (asymptotically
valid) p-values, ranking methods by the average log p-value. We hope that the model and bench-
mark spurs further development of tests for contamination, and encourage members of the research
community to improve on our results for low duplication counts.
4.2 S HARDING AND PERMUTATION COUNT
Our test relies on two parameters – the number of shards in the test, and the number of permutations
to sample. Both of these affect the power of the test, and we carefully study the impact of these
parameters on our ability to detect test sets by evaluating our pre-trained model on the 6 datasets that
contain 1000 examples (BoolQ, HellaSwag, MNLI, NaturalQuestions, TruthfulQA, PIQA). For the
number of shards, we explore a range of settings, from 10 shards to 200 shards and for permutations
we test a range from 1 to 50 permutations.
Shard sensitivity Our results in Figure 3a show that there is a sweet spot to the number of shards,
around 10-20 shards, where our detection rate for test sets are maximized. Larger numbers of shards
perform worse, since each shard involves fewer examples. Shards below 10 do not perform well, as
this is likely too few samples to merit the use of an asymptotically valid test like the t-test.
(a) So long as each shard contains enough exam-
ples and enough shards are used, the p-value is
stable under variations of the number of shards r.
We plot the average log p-value of those six of our
pre-trained model benchmarks with 1,000 exam-
ples, varying the number of examples per shard.
0 10 20 30 40 50
Permutations per Shard4.0
3.5
3.0
2.5
Average log p-value
Average log p-value vs. Permutations per Shard(b) Increasing the permutation count improves the
estimate of the mean log-likelihood of the shard
under permutation, but we find that the p-value
stabilizes at around 25 shuffles. We plot the aver-
age logarithm of the p-value(s) of 6 datasets eval-
uated on our pretrained model as a function of per-
mutations per shard.
Figure 3: Impact of varying shard and permutation counts on test performance.
Permutation count sensitivity We also measure the dependence of our test on the number of
permutations per shard in Figure 3b, and find more permutations to generally improve the power
of our test. We test permutations of 1, 2, 10, 25, 50 and compute the average log p-value of the 6
datasets evaluated on the pretrained model. In practice we find that there is substantial diminishing
returns beyond 25 permutations in the t-test. This stands in stark contrast to the permutation test,
where a permutation count of 25 would only allow for a minimum p-value of 0.038.
4.3 E VALUATING EXISTING MODELS FOR DATASET CONTAMINATION
We now demonstrate the utility of our procedure in validating test set contamination in multiple
publicly available language models: LLaMA2 (Touvron et al. (2023)), Mistral-7B (Mistral (2023)),
Pythia-1.4B (Biderman et al. (2023)), and GPT-2 XL (Radford et al. (2018)), on eight public test
benchmarks: AI2-Arc (Clark et al. (2018)), BoolQ (Clark et al. (2019)), GSM8K (Cobbe et al.
2
8(2021)), LAMBADA (Paperno et al. (2016)), NaturalQA (Kwiatkowski et al. (2019b)), Open-
BookQA (Mihaylov et al. (2018a)), PIQA (Bisk et al. (2019)), and MMLU (Hendrycks et al. (2021)).
Computationally, we find that our test runs reasonably quickly for a 7 billion parameter model, al-
lowing for the testing of 65 files for contamination in under 24 hours using 50 shards with 250
permutations per shard, and we find that the test outcomes are in general agreement with the con-
tamination study results of Brown et al. (2020c) and Touvron et al. (2023): we do not find evidence
of pervasive verbatim test set contamination across the models and benchmarks we tested.
Table 2: P-values for contamination tests on open models and benchmarks. With the exception of
ARC for Mistral, none of the tests give evidence for contamination. The MMLU results are marked
with a †to indicate that the p-values are the result of p-value aggregating the constituent datasets of
MMLU after heuristic filtering for non-exchangable datasets (see main text). The resulting LLaMA2
and Mistral p-values are small, consistent with the contamination studies in Touvron et al. (2023)
identifying mild MMLU contamination.
Dataset Size LLaMA2-7B Mistral-7B Pythia-1.4B GPT-2 XL BioMedLM
Arc-Easy 2376 0.318 0.001 0.686 0.929 0.795
BoolQ 3270 0.421 0.543 0.861 0.903 0.946
GSM8K 1319 0.594 0.507 0.619 0.770 0.975
LAMBADA 5000 0.284 0.944 0.969 0.084 0.427
NaturalQA 1769 0.912 0.700 0.948 0.463 0.595
OpenBookQA 500 0.513 0.638 0.364 0.902 0.236
PIQA 3084 0.877 0.966 0.956 0.959 0.619
MMLU†– 0.014 0.011 0.362 – –
We tested five models for contamination by eight publicly available benchmarks and list the results in
Table 2. We use 50 shards and 250 permutations per shard throughout the experiments. For test sets
containing more than 5,000 examples, we truncate and test only the first 5,000. Benchmark datasets
in the wild may contain non-exchangable elements such as sequential indexes or duplicate examples
which would break the false positive guarantees of our test. To check for these possibilities we
manually inspected each dataset for non-exchangable elements, and also run our tests on a ‘negative
control’ of BioMedLM (Bolton et al., 2022), a language model trained exclusively on PubMed data
and known not to contain the benchmarks used here. The p-values computed for BioMedLM are not
significant across the benchmarks shown here, suggesting that any significant results for the other
models tested are not simply due to non-exchangeability.
Our results in Table 2 show non-significant results across most models and datasets. While failure to
reject the null hypothesis is not direct evidence in favor of the null, our synthetic canary evaluations
from section 4.1 suggest that it is unlikely for these models to have seen most of these test sets
more than 2 to 10 times. One notable exception is AI2-ARC on Mistral, which does return a low
p-value of 0.001 and could suggest some contamination. While this p-value appears small, we
caution the reader that applying a multiple hypothesis test corection would imply that 0.001 is right
at the boundary of statistical significance, and due to challenges with garden-of-forking-paths type
analysis, significance tests that are at the boundary of the rejection cutoff should be interpreted
cautiously. We present these results as showing promising first steps towards third-party audits of
test set contamination, rather than a direct proof of contamination of specific models.
We additionally discuss contamination tests on MMLU, which was identified as a potential con-
taminant in recent studies (Touvron et al. (2023)), and involves important details. MMLU is not
a single test set, but rather a collection of test sets. We speculate that at least 14 of the test sets
are non-exchangeable, and applying our test directly would break the false positive guarantees. To
understand if our tests can still provide some insights into contamination, we run our MMLU test
with a non-exchangability filtering criterion.
To evaluate models for contamination by MMLU, we first exclude those 14 test files from con-
sideration for which our test flags either BioMedLM or GPT-2 as contaminated (both are negative
controls as the GPT-2 family of models predates MMLU). We run our test on each of the 43 remain-
ing test files (with 1000 permutations, due to the small size of each file) and aggregate the p-values
9using Fisher’s method (Fisher (1934)). Although the omnibus p-values resulting from this proce-
dure can no longer provide a proof of contamination (due to non-independence and heuristic nature
of the fitering step), their magnitude serves as heuristic evidence for contamination. The resulting
p-values and empirical CDFs (Figure 4) of the 43 test sets are indicate mild deviation from the null
hypothesis, consistent with the findings of mild test set contamination in Touvron et al. (2023).
5 R ELATED WORK
Our work relates to a large literature on data memorization, privacy, and membership inference
attacks for large language models. We discuss some of the most relevant works to ours below.
There is a substantial literature studying memorization of data in large language models, often from
the privacy perspective (Carlini et al., 2021; 2019; Kandpal et al., 2022; Mattern et al., 2023; Carlini
et al., 2023). Most of these works have focused on analyses of what is memorized and whether pri-
vate information can be extracted from a large language model, but do not build tests to specifically
identify test set contamination. Our work has a narrower focus on test set contamination, but this
also allows us to build tests that provide more precise guarantees of contamination.
Data contamination has been studied in many contexts, including in the study of pretraining corpora
((Dodge et al., 2021)) as well as in the analysis section of many language model papers (Hoffmann
et al., 2022; Brown et al., 2020a; Gao et al., 2020). The n-gram based analyses in these papers can
shed light on contamination, but they can have high false positives (e.g. SQuAD (Rajpurkar et al.,
2016) containing Wikipedia) and are limited to those datasets chosen for analysis. Our approach
enables third party tests of dataset contamination with only access to log probabilities, enabling
broader testing, without having to trust the model provider.
For third-party tests of contamination, there have been a few recently proposed heuristics. Sainz et al.
(2023) propose to identify test set contamination in GPT-3 and GPT-4 by prompting the models to
generate verbatim examples from a test set. The contemporaneous work of Golchin & Surdeanu
(2023) similarly proposes to identify contamination in black-box models by prompting a model to
generate completions of random example prefixes and using GPT-4 to judge the closeness between
the completion and the ground truth. Min-k%-Prob (Shi et al. (2024)) indicates contamination when
the average of the k% tokens with lowest probablity in the test set is high. We report the results of
Min-k%-Prob using the same setup as in the canary experiments of section 4.1 (see the appendix),
and find strong and reliable detection for a duplication count of 10 and above, and detection at
duplication counts as low as 4. We note that while these approaches are efficient and do not require
access to pre-training data, they do not enjoy the same provable false-positive guarantees of our
work.
Closest to our work is the exposure statistic in Carlini et al. (2019) and other subsequent variations
(Mattern et al. (2023)), which tests the perplexity differences between a target sequence and random
sequences. The idea of comparing the rank of the target log probability to some baseline distribution
is similar to our work. However, our work is distinct in using the exchangability of datasets to obtain
an exact null distribution (giving us provable guarantees when identifying contamination) and in
developing a sensitive and efficient shard-based test.
Beyond language modeling, identifying the presence of a particular set of examples in the training
data of a machine learning model is related to the security and privacy topic of membership inference
(Shokri et al. (2017); Mattern et al. (2023)). Our work contributes to this literature by developing a
new form of membership inference attack that leverages the exchangability of benchmark datasets.
6 L IMITATIONS
We highlight a few limitations of our approach for detecting test set contamination. First, the p-
values presented in this paper do not have multiple test corrections applied, as it is difficult to define
the ‘total number of hypotheses’ tested throughout development.
Second, any application of this test in practice will likely involve taking an off-the-shelf benchmark
dataset X, for which it will be difficult to know if the dataset is truly exchangable. Heuristic negative
controls such as our BioMedLM experiments can be helpful, but we cannot ever prove that a dataset
10is exchangable without knowing its data generating process. We strongly encourage future dataset
creators to apply a random shuffle to their datasets (and to publicize this fact), which would allow
our tests to be applied.
Finally, our tests focus on the case of verbatim contamination where a language model ingests a
test set directly. Contamination can happen in many other ways, such as when a language model
consumes a data source used in the construction of a benchmark (e.g. Wikipedia used in SQuAD,
professional tests in MMLU). Verbatim memorization of a test set is not the only form of contami-
nation, and our test cannot rule out the possibility of more complex forms of partial contamination.
7 C ONCLUSION
In this work, we demonstrated that it is possible to construct a statistical test for test set contam-
ination that provides false positive rate guarantees and requires nothing other than the ability to
compute log probabilities. We construct new, sharding based tests for contamination and demon-
strate their power on both carefully constructed canaries as well as publically available language
models. We view these tests as a first step towards building powerful third party tests of contamina-
tion, and we believe it is an exciting open problem to build tests that are capable of reliably detecting
contamination at the single-duplication-count regime.
8 A CKNOWLEDGEMENTS
We gratefully acknowledge support from the CRFM Levanter team, especially David Hall, for
both computational resources and infrastructure support, and to Google’s TPU Research Cloud
(TRC) for Cloud TPUs used in the pretraining experiments. Nicole Meister was supported by NSF
GRFP DGE-2146755. Niladri Chatterji and Faisal Ladhak were supported by SAIL and Tatsunori
Hashimoto was supported by a gift from IBM and a Hoffman-Yee grant. Finally, we would like
to thank Nicholas Carlini and Percy Liang for insightful discussions on memorization and test set
contamination.
REFERENCES
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson
Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernan-
dez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson,
Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Ka-
plan. Training a helpful and harmless assistant with reinforcement learning from human feedback,
2022.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya
Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language
models across training and scaling, 2023.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning
about physical commonsense in natural language. ArXiv , abs/1911.11641, 2019. URL https:
//api.semanticscholar.org/CorpusID:208290939 .
Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang.
Biomedlm, 2022. URL https://crfm.stanford.edu/2022/12/15/biomedlm.
html .
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models
are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS) , 2020a.
11T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,
J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models
are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020b.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020c.
N. Carlini, C. Liu, ´U. Erlingsson, J. Kos, and D. Song. The secret sharer: Evaluating and testing
unintended memorization in neural networks. In USENIX Conference on Security Symposium ,
SEC’19, pp. 267–284, USA, 2019. USENIX Association. ISBN 9781939133069.
N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. B. Brown,
D. X. Song, ´U. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language
models. In USENIX Security Symposium , 2021.
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and
Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint
arXiv:2202.07646 , 2023.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 , 2022.
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Jill
Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers) , pp. 2924–2936, Minneapolis, Min-
nesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL
https://aclanthology.org/N19-1300 .
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. ArXiv , abs/1803.05457, 2018. URL https://api.semanticscholar.org/
CorpusID:3922816 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
Jesse Dodge, Maarten Sap, Ana Marasovi ´c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,
Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the
colossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods
12in Natural Language Processing , pp. 1286–1305, Online and Punta Cana, Dominican Republic,
November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.
98. URL https://aclanthology.org/2021.emnlp-main.98 .
R. A. Fisher. Statistical Methods for Research Workers . Oliver & Boyd, Edinburgh, 4th edition,
1934.
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N. Nabeshima, S. Presser, and C. Leahy. The Pile: An 800gb dataset of diverse text for lan-
guage modeling. arXiv preprint arXiv:2101.00027 , 2020.
Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large
language models. arXiv preprint arXiv:2308.08493 , 2023.
David Hall, Ivan Zhou, and Percy Liang. Levanter — legible, scalable, reproducible foun-
dation models with jax, 2023. URL https://crfm.stanford.edu/2023/06/16/
levanter-1_0-release.html .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-
tional Conference on Learning Representations (ICLR) , 2021.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,
L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,
B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre.
An empirical analysis of compute-optimal large language model training. In Advances in Neural
Information Processing Systems (NeurIPS) , 2022.
Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates pri-
vacy risks in language models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Con-
ference on Machine Learning , volume 162 of Proceedings of Machine Learning Research ,
pp. 10697–10707. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/
v162/kandpal22a.html .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion
Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav
Petrov. Natural questions: A benchmark for question answering research. Transactions of the
Association for Computational Linguistics , 7:452–466, 2019a. doi: 10.1162/tacl a00276. URL
https://aclanthology.org/Q19-1026 .
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N.
Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav
Petrov. Natural questions: a benchmark for question answering research. Transactions of the
Association of Computational Linguistics , 2019b.
E. L. Lehmann and Joseph P. Romano. Testing statistical hypotheses . Springer Texts in Statistics.
Springer, New York, third edition, 2005. ISBN 0-387-98864-5.
Yuanzhi Li, S ´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.
Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 , 2019.
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human
falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguis-
tics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.
acl-long.229 .
Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Pro-
ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers) , 2022.
13Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan,
and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neigh-
bourhood comparison. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),
Findings of the Association for Computational Linguistics: ACL 2023 , pp. 11330–11343,
Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.
findings-acl.719. URL https://aclanthology.org/2023.findings-acl.719 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Conference on Empirical Methods
in Natural Language Processing , 2018a. URL https://api.semanticscholar.org/
CorpusID:52183757 .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Ellen Riloff, David Chiang,
Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing , pp. 2381–2391, Brussels, Belgium, October-
November 2018b. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL
https://aclanthology.org/D18-1260 .
Mistral. Mistral 7b, 2023. URL https://mistral.ai/news/
announcing-mistral-7b/ .
OpenAI. Gpt-4 technical report, 2023.
Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The lambada dataset:
Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.
Belinda Phipson and Gordon K Smyth. Permutation p-values should never be zero: Calculating
exact p-values when permutations are randomly drawn. Statistical Applications in Genetics and
Molecular Biology , 9(1), 2010. doi: doi:10.2202/1544-6115.1585. URL https://doi.org/
10.2202/1544-6115.1585 .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners, 2018. URL
https://d4mucfpksywv.cloudfront.net/better-language-models/
language_models_are_unsupervised_multitask_learners.pdf .
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions
for machine comprehension of text. In Jian Su, Kevin Duh, and Xavier Carreras (eds.), Pro-
ceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp.
2383–2392, Austin, Texas, November 2016. Association for Computational Linguistics. doi:
10.18653/v1/D16-1264. URL https://aclanthology.org/D16-1264 .
Oscar Sainz, Jon Ander Campos, Iker Garc ´ıa-Ferrero, Julen Etxaniz, and Eneko Agirre.
Did chatgpt cheat on your test?, 2023. URL https://hitz-zentroa.github.io/
lm-contamination/blog/ .
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi
Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv
preprint arXiv:2310.16789 , 2024.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference at-
tacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP) ,
2017.
Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
URL https://github.com/togethercomputer/RedPajama-Data .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
14Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task bench-
mark and analysis platform for natural language understanding. In International Conference on
Learning Representations (ICLR) , 2019.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for
sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent
(eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp.
1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-1101. URL https://aclanthology.org/N18-1101 .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a
machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Associ-
ation for Computational Linguistics , pp. 4791–4800, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://www.aclweb.org/
anthology/P19-1472 .
15APPENDICES
A S TRIDED LOG-LIKELIHOODS
To compute log-likelihoods for sequences exceeding the context length, we use a strided window
approach, with a stride equal to half of the model’s context length. We find that decreasing the stride
beyond half the context length does not yield significant gains.
B P RETRAINING DETAILS
We elaborate on the hyperparameters and training procedure of our 1.4B language model, trained
from scratch on intentionally contaminated Wikitext.
We use a GPT-2 architecture with 1.4B parameters, with the architecture hyperparameters given by
a hidden dimension of 1536, 24 heads, 48 layers, and a sequence length of 2048. The training batch
size was 256. Based on the number of training tokens, sequence length, and training batch size, we
trained this model for 46000 steps so as to consume the tokens in our mixture datasets exactly once.
The model was optimized using AdamW with a learning rate of 1e-4 and weight decay of 0.1. We
trained the model using Levanter on a v3-128 TPU instance on Google Cloud for 1.5 days (Hall
et al. (2023)).
C 10 C ANARY DATASETS
In this section we provide additional details on the 10 canary datasets we injected into Wikitext
to form our pretraining data. For BoolQ3(Clark et al., 2019), HellaSwag4(Zellers et al., 2019),
MNLI5(Williams et al., 2018), Natural Questions6(Kwiatkowski et al., 2019a), TruthfulQA7(Lin
et al., 2022), PIQA8(Bisk et al., 2019), we sample a random subset of 1000 examples. For Open-
bookQA9(Mihaylov et al., 2018b), because of its smaller test set of size n=500, we used all 500
examples. Finally, for MMLU10(Hendrycks et al., 2021), we chose from subsets with no multi-line
examples and having at least 500 examples, specifically Professional Psychology (n=611), MMLU
Professional Law (n=1000), MMLU High School Psychology (n=544). Finally, we shuffle the ex-
amples in all datasets to make them exchangeable. In Table 3, we provide additional information
about the injected datasets including number of examples, average words per example, and number
of tokens per dataset. For each duplication rate, we included a short, medium and longer dataset, as
measured by the total token count. The total token count of injected benchmarks is 19.235M tokens,
meaning that the injected dataset is less than 0.1% of the entire pre-training dataset.
D F ULL MMLU R ESULTS
We list in table 4 the full set of MMLU results used to generate the omnibus p-values for contami-
nation by MMLU listed in table 2, before filtering out for suspected non-exchangeability.
E C OMPARISON TO MIN-K%-P ROB BASELINE
We provide a comparison against Min-k% Prob (Shi et al. (2024)), a state of the art heuristic method
for contamination detection proposed contemporaneous to our work. We use the same pretrained
model and test sets from our experiments in Section 4.1.
Our implementation of Min-k%-Prob follows the methodology outlined in Shi et al. (2024); we run
the method on one hundred 512-token spans sampled from each benchmark, and tune the decision
3https://github.com/google-research-datasets/boolean-questions
4https://rowanzellers.com/hellaswag/
5https://cims.nyu.edu/ ˜sbowman/multinli/
6https://github.com/google-research-datasets/natural-questions
7https://github.com/sylinrl/TruthfulQA/blob/main/data/finetune_truth.
jsonl
8https://yonatanbisk.com/piqa/
9https://allenai.org/data/open-book-qa
10https://github.com/hendrycks/test
16Table 3: Injected canary datasets and duplication counts used in our pretraining experiments.
Name Examples Avg Words/Ex Tokens Dup Rate (High) Dup Rate (Low)
BoolQ 1000 110 110k 1 1
HellaSwag 1000 185 185k 1 1
OpenbookQA 500 40 20k 1 2
Natural Questions 1000 32 32k 10 2
MNLI 1000 235 235k 10 4
TruthfulQA 1000 25 25k 10 4
PIQA 1000 50 50k 50 7
MMLU Pro. Law 1000 2000 200k 50 7
MMLU Pro. Psych 611 50 30k 50 –
MMLU H.S. Psych 544 37 20k 100 –
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0LLaMA2-7B
ECDF
Uniform
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Mistral-7B
ECDF
Uniform
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0Pythia-1.4B
ECDF
Uniform
Figure 4: Empirical CDFs of MMLU p-values of LLaMA2, Mistral, and Pythia after exclusion of
BioMedLM and GPT-2 significant test files, plotted against CDFs of a Uniform(0,1).
threshold on a validation set of five of our contaminated test sets, and five test sets not used in
our data mixture. The threshold is tuned for a false positive rate of 5% to allow for a meaningful
comparison against our test. A value of k=20 is used as is recommended in the paper.
17Table 4: Full MMLU results for LLaMA2-7B, Mistral-7B, Pythia-1.4B, GPT-2XL and BioMedLM.
Dataset Size LLaMA2-7B Mistral-7B Pythia-1.4B GPT-2 XL BioMedLM
Abstract-Algebra 100 0.103 0.246 0.645 0.894 0.861
Anatomy 135 0.586 0.605 0.605 0.439 0.649
Astronomy 152 0.550 0.657 0.050 0.003 0.050
Business-Ethics 100 0.936 0.440 0.107 0.808 0.499
Clinical-Knowledge 265 0.199 0.108 0.081 0.004 0.268
College-Biology 144 0.108 0.051 0.152 0.260 0.779
College-Chemistry 100 0.526 0.614 0.595 0.355 0.265
College-Computer-Science 100 0.060 0.498 0.383 0.532 0.267
College-Mathematics 100 0.059 0.151 0.162 0.321 0.727
College-Medicine 173 0.397 0.106 0.340 0.440 0.067
College-Physics 102 0.694 0.757 0.972 0.719 0.262
Computer-Security 100 0.214 0.007 0.314 0.180 0.928
Conceptual-Physics 235 0.554 0.333 0.811 0.710 0.924
Econometrics 114 0.616 0.761 0.540 0.508 0.035
Electrical-Engineering 145 0.266 0.364 0.595 0.490 0.277
Elementary-Mathematics 378 0.059 0.416 0.260 0.355 0.528
Formal-Logic 126 0.666 0.750 0.990 0.930 0.398
Global-Facts 100 0.779 0.957 0.448 0.339 0.667
High-School-Biology 310 0.645 0.279 0.476 0.499 0.416
High-School-Chemistry 203 0.229 0.426 0.813 0.539 0.055
High-School-Computer-Science 100 0.023 0.085 0.106 0.045 0.484
High-School-European-History 165 0.009 1e-38 1e-38 1e-38 1e-38
High-School-Geography 198 0.194 0.339 0.341 0.721 0.277
High-School-Government-And-Politics 193 0.294 0.066 0.025 0.372 0.003
High-School-Macroeconomics 390 0.543 0.222 0.276 0.236 0.511
High-School-Mathematics 270 0.473 0.182 0.001 0.272 0.032
High-School-Microeconomics 238 0.862 0.797 0.712 0.122 0.181
High-School-Physics 151 0.339 0.757 0.171 0.114 0.354
High-School-Psychology 545 0.033 0.037 0.004 0.039 0.007
High-School-Statistics 216 0.077 0.119 0.228 0.233 0.211
High-School-Us-History 204 1e-38 1e-38 1e-38 1e-38 0.001
High-School-World-History 237 1e-38 1e-38 1e-38 1e-38 1e-38
Human-Aging 223 0.882 0.830 0.617 0.677 0.803
Human-Sexuality 131 0.807 0.959 0.502 0.789 0.471
International-Law 121 0.061 0.545 0.532 0.582 0.817
Jurisprudence 108 0.009 0.075 0.852 0.879 0.616
Logical-Fallacies 163 0.103 0.188 0.082 0.397 0.193
Machine-Learning 112 0.266 0.297 0.373 0.061 0.868
Management 103 0.516 0.209 0.454 0.907 0.281
Marketing 234 0.874 0.684 0.977 0.848 0.451
Medical-Genetics 100 0.501 0.037 0.523 0.425 0.729
Miscellaneous 783 0.099 0.122 0.086 0.266 0.081
Moral-Disputes 346 0.017 0.011 0.097 0.190 0.157
Moral-Scenarios 895 0.652 0.022 0.121 0.487 0.413
Nutrition 306 0.163 0.601 0.413 0.865 0.609
Philosophy 311 0.011 0.094 0.003 0.049 0.044
Prehistory 324 0.203 0.412 0.055 0.268 0.262
Professional-Accounting 282 0.708 0.242 0.197 0.910 0.798
Professional-Medicine 272 0.001 0.017 0.001 0.002 1e-38
Professional-Psychology 612 0.001 0.001 0.058 0.009 0.201
Public-Relations 110 0.402 0.237 0.162 0.512 0.622
Security-Studies 245 0.070 0.043 0.687 0.061 0.073
Sociology 201 0.350 0.258 0.260 0.660 0.496
Us-Foreign-Policy 100 0.934 0.252 0.778 0.646 0.173
Virology 166 0.203 0.854 0.219 0.796 0.212
World-Religions 171 0.311 0.882 0.933 0.222 0.851
18Table 5: Canary results for Min-k%-Prob.
Name Size Dup Count Min-k% Prob
BoolQ 1000 1 13%
HellaSwag 1000 1 2%
MNLI 1000 10 100%
MMLU Pro. Law 1533 50 100%
MMLU H.S. Psychology 544 100 100%
19
user: What is the problem that the paper aims to address?
assistant: The paper aims to address the problem of test set contamination in large language models (LLMs). Test set contamination occurs when language models, during training, inadvertently ingest evaluation benchmark datasets, which are supposed to be kept separate from the training data. This contamination can lead to inflated performance metrics, as the models may memorize answers rather than generalize from learned patterns. The main challenge is detecting such contamination in proprietary language models, where the pretraining data is often not publicly accessible. The paper proposes a method to provide provable guarantees of test set contamination without needing access to pretraining data or model weights, enabling independent verification of potential contamination in black-box models.
user: What are the claimed limitations in the state of the art that the paper addresses? Are the limitations real?
assistant: The paper identifies several limitations in the current state of the art regarding detecting test set contamination in language models:

1. **Lack of Access to Training Data**: Many existing approaches require access to the training data or model weights to detect contamination, which is often not possible with black-box models provided through APIs or proprietary systems. 

2. **Heuristic Methods**: Current methods often rely on heuristics or indirect metrics like n-gram overlap, which can lead to false positives or less reliable results because they do not provide provable guarantees of contamination.

3. **Inability to Provide Proof**: Existing methods may generate speculation about data contamination but do not offer irrefutable or statistically significant evidence of contamination.

4. **Computational Inefficiency**: Some methods, particularly those needing a large number of computations to assess the likelihood of contamination, are computationally expensive and less practical for real-world applications.

5. **Limited Benchmark Scope**: Many methods are limited to a select set of benchmarks and do not generalize well to other datasets or models.

The paper's proposed approach aims to address these limitations by developing a statistical test that can work with only log probability queries from the model, providing provable false positive rate guarantees that do not rely on accessing the model's training data or parameters.

Regarding whether these limitations are real:
- **Lack of Access and Need for Proof**: Indeed, it is a real limitation, especially given the proprietary nature of many leading LLMs where data access is restricted.
- **Reliance on Heuristics**: Current methods often lack rigorous statistical backing, which the field recognizes as a limitation.
- **Computational Efficiency**: While not universally recognized as a limitation, efficient methods that can provide rapid assessments are certainly valuable.
- **Scope**: The limitation concerning the scope of benchmarks addressed by existing techniques is valid, as the diversity of potential test datasets complicates the assessment.

In summary, the described limitations are generally reflective of real challenges faced in the field of AI research, particularly in verifying data contamination in black-box settings.
user: What is the key novel technical contribution in the paper? Is it really novel?
assistant: The key novel technical contribution of the paper is the development of a statistical test for detecting test set contamination in black-box language models by leveraging the concept of exchangeability. This approach allows for the detection of contamination without requiring access to the model's training data or internal parameters. Specifically, the paper introduces a method where the likelihoods of different orderings of a benchmark dataset are compared. In cases where the model has seen the benchmark during training, it is hypothesized to assign higher likelihoods to the canonical ordering of the data. This conceptual framework allows for a statistical test that can make inferences about contamination with provable control over false positive rates.

Regarding the novelty:
- **Exchangeability**: The use of exchangeability to detect contamination is a novel application in this context. While exchangeability is a known statistical property, its application to prove test set contamination in the setting of large language models is creative and innovative.
- **False Positive Guarantees**: The paper's method offers a novel way to provide statistical guarantees for the rate of false positives, which addresses a gap left by previous heuristic methods.
- **Efficient Sharded Test**: The development of a computationally efficient sharded hypothesis test to improve power and efficiency is also a notable contribution.

Whether these contributions are entirely novel would depend on prior work in similar domains. The use of statistical properties like exchangeability in hypothesis testing is a known concept, but its specific application and adaptation to this problem of test set contamination appear unique, as the paper does not cite other works employing this strategy for similar contamination detection tasks. In that sense, the contribution is presented as novel within the scope of language model contamination analysis. However, thorough literature checks would be necessary to fully establish the novelty against all prior works.
user: Is the technical approach sound and clearly described? Are there any errors, unstated assumptions, or missing details?
assistant: The technical approach presented in the paper appears to be sound in its overall conceptual framework, but there are several aspects worth scrutinizing for potential issues, unstated assumptions, or missing details:

1. **Soundness**:
   - The approach of using exchangeability seems theoretically justifiable. Using this property for permutation tests leverages a well-understood statistical principle that, under exchangeability, all permutations of a dataset should be equally likely.

2. **Clarity**:
   - The paper describes its methodology by outlining how permutation tests are conducted and how a sharded likelihood test is introduced to manage computational resources while improving statistical power. This is relatively clear in terms of its procedural aspects.

3. **Assumptions**:
   - The key assumption is that the datasets in question are exchangeable. This means every permutation of the dataset is statistically indistinguishable which might not hold in practical scenarios, particularly for datasets deliberately ordered to follow certain logical progressions.
   - Another assumption is that seeing and memorizing a dataset leads to greater likelihood only in its canonical form, which might not hold if the learning process affects the memory and generalization capabilities in more nuanced ways.

4. **Potential Limitations**:
   - A critical point not deeply discussed is the potential for systematic biases in language models that might affect the handling of the "canonical" dataset order versus shuffled datasets independent of contamination, especially related to training artifacts that influence likelihood differently.
   - The paper might not make explicit how it handles edge cases, such as very small datasets, which might not provide enough permutation variance to yield a meaningful test.

5. **Missing Details**:
   - The paper introduces a "sharded" test approach but may require more elaboration on how sharding decisions are made and balanced for datasets with varying lengths.
   - Details about computational efficiency and scalability need clarification, especially for very large models where log-likelihood computations even for shards can be computationally demanding.

6. **Empirical Evaluations**:
   - While empirical results are provided, further robustness checks or comparisons with baseline methods (e.g., purely heuristic methods) under various conditions would strengthen claims about its efficacy and usability in real-world scenarios.

Overall, the paper appears to make rational proposals built on sound statistical principles. Still, like many theoretical exercises applied practically, its full utility would depend on empirical validation and real-world applicability assessments, taking into account all assumptions and edge cases.
user: Does the empirical evaluation include appropriate baselines and comparisons to validate the proposed approach compared to the state of the art?
assistant: The empirical evaluation in the paper is primarily focused on demonstrating the effectiveness of the proposed method using controlled experiments with intentionally contaminated datasets and some real-world language models. However, the paper does not provide a comprehensive comparison with existing state-of-the-art methods or other baseline approaches. Here's a more detailed assessment:

1. **Controlled Experiments**: 
   - The paper introduces canary datasets with known contamination levels to test the sensitivity of their method. This is a good practice to validate the basic efficacy of the proposed technique, showing its ability to detect contamination under controlled conditions.

2. **Real-world Models**:
   - The method is applied to several publicly available language models to demonstrate its applicability in real-world scenarios. This also serves to suggest that the method is computationally feasible for large models.

3. **Baseline Comparisons**:
   - The paper does not make extensive comparisons with existing heuristic methods used for contamination detection. This is a gap, as it would be crucial to benchmark the proposed method against other techniques to truly validate its superiority or complementary value.
   - The paper does mention the contemporaneous work by Shi et al. (2024) on "Min-k%-Prob," which is described in an appendix, but does not delve into detailed comparisons or analysis of potential differences in performance or applicability. Including such a comparison would strengthen the claim about the method's efficacy and novelty.

4. **Extended Validation**:
   - While the paper claims improvements in detecting contamination, it would benefit from tests against more diverse datasets or scenarios where the assumptions of exchangability might not hold, which would showcase robustness against edge cases or limitations of alternative methods.

5. **Discussion on Previous Limitations**:
   - The paper includes a brief discussion on prior techniques and their limitations but could integrate those findings with direct empirical demonstrations showing where those methods fall short in comparison to the proposed methodology.

Overall, while the evaluation suggests the proposed method is promising, the lack of comprehensive baseline comparisons with other methods in the field is a significant omission. Including such comparisons would help validate claims about the methodological advantages and provide a clearer context for the paper's contributions relative to the state of the art.
user: Does the empirical evaluation report well-established and reasonable metrics?
assistant: The empirical evaluation in the paper employs several reasonable metrics for assessing the efficacy of the proposed method for detecting test set contamination. Here's a detailed analysis:

1. **Statistical Significance (p-values)**:
   - The primary metric used in the paper is the p-value from the statistical tests (permutation test and sharded likelihood comparison test). Reporting p-values is a well-established practice in hypothesis testing to determine statistical significance and make inferences about the null hypothesis.

2. **False Positive Rate Control**:
   - The paper claims that its method provides provable false positive rate guarantees, which is a critical metric for evaluating the reliability of hypothesis tests in statistical contexts.

3. **Detection Sensitivity**:
   - The evaluation discusses the method's sensitivity to different duplication rates, showing how effectively the method can detect contamination at various levels. This sensitivity analysis is crucial for understanding the method's practical applicability.

4. **Log p-values**:
   - By examining the log p-values, the paper helps assess the power of detection, especially for situations where very low p-values are expected or required, like when accounting for multiple comparisons.

5. **Empirical Cumulative Distribution Function (ECDF)**:
   - The paper uses ECDFs to visualize the distribution of p-values across different datasets and models, providing a useful visual metric to intuitively understand the variance and trends in detection.

However, there is room for additional metrics that could enhance the evaluation:

- **Comparison Metrics**: Including metrics that compare detection performance against baseline or competing methods, such as recall, precision, or more contextual performance metrics, would be beneficial for comprehensive evaluation.
- **Computation Cost**: Empirical evaluation of computation time and resources required could further ground the potential applicability assumptions, particularly for scalability to larger models or datasets.

Overall, while the paper employs strong and well-established metrics for hypothesis testing, additional metrics and analyses, particularly involving comparisons with other methodologies, could provide a more rounded view of the method's performance and resource demands.
user: Are the evaluation benchmarks and datasets appropriately chosen? Are there any better benchmarks or datasets that should have been used?
assistant: The paper uses a combination of synthetic and real-world datasets to evaluate the proposed method, which is generally appropriate for demonstrating the technique's capability to detect test set contamination. Here's an analysis of the choices made and possible improvements:

1. **Synthetic Contaminated Datasets**:
   - By intentionally contaminating canary datasets with different duplication rates, the authors create controlled scenarios to test the sensitivity and robustness of their method. This approach is detailed and allows for fine-grained control over experimental conditions, which is a strength of the evaluation.

2. **Real-world Language Models**:
   - The evaluation includes tests on several publicly available language models (e.g., LLaMA2, Mistral-7B, Pythia-1.4B, GPT-2 XL) using common benchmarks like BoolQ, PIQA, and others. This shows the general applicability of the method in detecting potential contamination in widely used LMs, making the evaluation relevant to real-world applications.

3. **Diversity of Benchmarks**:
   - The selection of benchmarks covers various task types, including question answering, commonsense reasoning, and natural language understanding. This diversity helps demonstrate the method's applicability across different domains.

Possible Improvements:

- **Additional Benchmarks**:
  - While the chosen benchmarks are diverse, including additional datasets known for specific challenges (e.g., datasets with known class imbalance or adversarial examples) might test the limits of the methodology under less ideal conditions.
  
- **Cross-Lingual Evaluation**:
  - The evaluation is focused primarily on English-language datasets. Including multilingual benchmarks could enhance the generalizability and applicability of the method across language-specific or cross-lingual tasks.

- **Legacy and Emerging Datasets**:
  - Assessing the methodology against both established benchmarks and recently released ones might provide insights into its adaptability to evolving datasets and emerging trends in dataset construction and characteristics.

In summary, while the paper has made appropriate choices for datasets and benchmarks, there is always room to widen the evaluation scope, ensuring robustness and relevance in an even broader range of scenarios and challenges.
user: Do the empirical results really support the claims of the paper?
assistant: The empirical results presented in the paper provide support for several key claims, but there are areas where the connection between results and claims could be strengthened:

1. **Detection of Contamination**:
   - The paper's experiments with canary datasets show that the proposed method can detect dataset contamination at various duplication rates. This supports the claim that the method can effectively identify contamination even with modest duplication rates.

2. **Sensitivity and False Positive Rate**:
   - The results demonstrate that the method provides statistical significance (p-values) and false positive rate control, aligning with the claim of provable guarantees. The use of permutation tests and exchangeability contributes to this aspect and is supported by the empirical evidence.

3. **Application to Real-world Models**:
   - Testing the method on publicly available language models (e.g., LLaMA2, Mistral-7B, Pythia-1.4B, GPT-2 XL) suggests that it can be applied to real-world situations. This supports the claim of the method's practicality in existing language models.

4. **Comparison to Baselines**:
   - The paper lacks comprehensive comparisons to previously existing methods or explicit baselines (e.g., heuristic methods). While it compares briefly with contemporaneous work ("Min-k%-Prob"), the empirical results would be more compelling with detailed benchmarks against state-of-the-art methods.

5. **Robustness and Scalability**:
   - While the empirical results indicate the method's ability to scale to large models, explicit metrics or comparisons on computational efficiency and scalability are not deeply explored. This can partially limit claims about the method's efficiency and utility "in the wild."

Overall, the results support the claims of effective detection and statistical guarantees to some extent. However, to robustly support all claims, especially relative advantages over existing methods, the work would benefit from more comprehensive empirical comparisons and demonstrations under varied conditions. Additionally, addressing scaling considerations and confirming broad applicability across diverse datasets would strengthen its claims further.
user: Now read your own analysis. Identify any logical gaps or unsubstantiated claims and correct them.
assistant: Here is a revised analysis addressing any potential logical gaps or unsubstantiated claims from the initial review:

1. **Detection of Contamination**: 
   - The initial analysis claims that the method effectively detects contamination even with modest duplication rates based on canary dataset experiments. However, it is crucial to specify that the detection was reliable mainly at higher duplication rates (10 or more). The detection capability at lower duplication rates remains less clear, and improvements would be necessary for detection with a duplication count of 1. The paper suggests some potential in this area but does not provide substantial proof for low duplication.

2. **Sensitivity and False Positive Rate**:
   - While the paper demonstrates p-values and claims statistical guarantees, it is additional to carefully validate that these are not influenced by implementation details or specific dataset characteristics. The claims about guarantees are theoretically valid but should account for any unstated assumptions regarding dataset exchangeability broadly.

3. **Application to Real-world Models**:
   - The claims regarding practicality are partially supported by tests on actual language models, but it would be more robust with additional varying conditions such as diverse dataset types (e.g., tasks, languages). The applicability should not solely rely on similarity to controlled experiments.

4. **Comparison to Baselines**:
   - The initial analysis describes a lack of comprehensive comparisons. While the paper does conduct a comparison with "Min-k%-Prob," this section should emphasize more clearly the need for benchmark-driven validations and direct comparisons with well-known approaches for stronger empirical backing.

5. **Robustness and Scalability**:
   - Claims about scalability and efficiency should have been more specific about experimental conditions. The initial review mentions a lack of explicit metrics, which suggests an area for potential strengthening through rigorous evaluations of runtime performance and operational feasibility, both of which are implicitly vital for real-world utility.

In conclusion, acknowledging these points ensures a more precise alignment between empirical results and the paper's claims. Addressing these gaps involves detailed empirical validation against more diverse datasets and conditions, stronger comparative benchmarks, and rigorous statistical consistency checks. The paper should also present findings under conditions that match both controlled and open-field environments to generalize better beyond initial tests.
user: If you were the harshest critic of this paper, what would your top concerns be? Provide a short bullet list of critiques.Name relevant SOTA papers or standard results from major AI conferences in the last 2-3 years that may relate to this approach. Evaluate whether the paper’s contributions outperform or differ significantly.Propose additional experiments or ablations that could strengthen the paper’s claims. How would these experiments be designed?
assistant: As a harsh critic of this paper, my top concerns would include:

1. **Lack of Comprehensive Baseline Comparisons**:
   - The paper does not provide a thorough comparison with existing state-of-the-art methods for detecting test set contamination, such as those based on heuristic methods or detailed memorization analyses.
   - It would benefit from more empirical comparisons to works like Carlini et al. (2021), which investigate data memorization in large language models, or Shi et al. (2024) with the Min-k%-Prob method.

2. **Assumptions on Exchangeability**:
   - The reliance on dataset exchangeability might not hold for all benchmark datasets, and this limitation is not thoroughly investigated or addressed in the paper.
   - There is insufficient discussion around how often real-world datasets conform to these assumptions and how non-exchangeability could affect results.

3. **Limited Real-World Dataset Testing**:
   - Testing on more diverse and complex real-world datasets, including multilingual data or tasks, would provide a better indication of the method's practical utility and robustness.

4. **Scalability and Efficiency**:
   - The paper lacks explicit discussion or metrics regarding the computational cost associated with the proposed tests, particularly for very large models or datasets.
   - Real-world applicability depends heavily on these factors, especially for practitioners with limited computational resources.

5. **Reproducibility and Open Benchmarking**:
   - While the paper discusses releasing data for future research, further details about the data, model implementation, and evaluation pipelines could enhance reproducibility.

Relevant SOTA Papers or Results:

- **Carlini et al. (2021)**: This work provides a thorough analysis of data memorization in language models and proposes methods to evaluate such memorization.
- **Shi et al. (2024)**: Proposes the Min-k%-Prob heuristic which is also aimed at detecting contamination and could serve as a relevant baseline.
- **Kandpal et al. (2022)**: Discusses deduplication's effect on language model memorization.

Evaluation Against SOTA:
- The contributions of this paper differ by focusing on providing provable false positive guarantees, unlike the heuristic nature of many current methods. However, rigorous comparisons are absent, limiting the evaluation of outperformance.

Proposed Additional Experiments:

1. **Comparative Baseline Experiments**:
   - Design experiments explicitly comparing the proposed method with Carlini et al. (2021) and Shi et al. (2024), measuring contamination detection rates across varying dataset conditions.
   - Utilize shared setup parameters (e.g., duplication rates) for a direct comparison of detection thresholds and computational costs.

2. **Ablation Studies**:
   - Conduct ablation studies on the number of shards and permutations, analyzing both computational efficiency and detection power.
   - Examine sensitivity to different dataset properties (e.g., noise and ordering).

3. **Non-Exchangeable Data Tests**:
   - Test the method on datasets partially ordered or with explicit biases to evaluate robustness against violations of the exchangeability assumption.
   - Include synthetic datasets designed to test resilience to such biases explicitly.

4. **Scalability Analysis**:
   - Measure the method's run-time and memory usage against larger models like GPT-3 and across diverse datasets, considering resource constraints.
   - Use profiling tools to identify computational bottlenecks and optimize potentially intensive operations.

Designing these experiments with a clear metric-focused evaluation would serve to substantiate the paper's claims and address limitations, thereby enhancing its contributions.
user: Please write a review of the paper, for possible inclusion at a top AI conference like AAAI or IJCAI. Remember as a top-tier venue, the conference only accepts papers that make significant contributions to the field, maintaining a rigorous standard of quality. Owing to the high standard for acceptance, only about the top 20% of the submissions have been accepted.
Your review should be structured as follows:

Title: [Title of the paper]
Summary: [Brief summary of the paper]
Strengths: [Strengths of the paper, including technical innovation, relation to state of the art, and empirical results]
Weaknesses: [Weaknesses of the paper, including technical errors, errors in methodology, missing citations, weaknesses in empirical evaluation (missing baselines, metrics, or experiments), etc.]
Suggestions for improvement: [Suggestions for improving the paper]
Overall recommendation: [strong accept, accept, borderline, reject, strong reject]
