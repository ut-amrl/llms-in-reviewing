Title: Proving Test Set Contamination in Black Box Language Models

Summary: 
This paper introduces a novel method to detect test set contamination in large language models without access to pretraining data or model weights. By leveraging the statistical property of exchangeability, the authors propose a test that compares the log probabilities of different orderings of a benchmark dataset. This approach allows for identifying potential contamination in black-box models and provides provable guarantees for false positive rates. The authors support their method with experiments on both controlled synthetic datasets and several real-world language models, suggesting that their technique is both sensitive and practical.

Strengths:
1. **Technical Innovation**: The method leverages exchangeability in a novel manner to provide statistically sound guarantees on contaminant detection, offering an innovative alternative to heuristic-based approaches.
2. **Provable Guarantees**: By providing false positive rate guarantees, the proposed technique addresses a significant limitation of many current heuristic methods.
3. **Empirical Evaluation**: The method is evaluated using both synthetic canary datasets and real-world language models, demonstrating its applicability in controlled and practical scenarios.

Weaknesses:
1. **Lack of Comprehensive Baseline Comparisons**: The paper lacks detailed comparisons with state-of-the-art methods, such as those described by Carlini et al. (2021) and Shi et al. (2024), which could support claims of outperformance or complementary advantages.
2. **Assumptions on Exchangeability**: The reliance on exchangeability is not thoroughly examined or validated on real-world datasets, leading to potential concerns over its broad applicability.
3. **Scalability and Computational Cost**: The paper lacks explicit discussion on the computational costs and scalability of the proposed method, especially with very large models or datasets.
4. **Reproducibility**: More details on the implementation and experimental setup could enhance reproducibility and facilitate further benchmarking efforts.

Suggestions for Improvement:
1. Include comprehensive baseline comparisons with existing methods to strengthen the validation of the proposed method's efficacy.
2. Conduct additional experiments on datasets with known ordering biases to assess the method's robustness against non-exchangeability.
3. Provide detailed analysis and metrics on computational efficiency and scalability, particularly for larger models.
4. Enhance reproducibility by providing more details on the experimental setup and making data and code easily accessible for community use.

Overall Recommendation: Borderline

While the paper introduces a novel and potentially impactful approach to detecting test set contamination with strong theoretical backing, the lack of comprehensive empirical comparisons and discussion surrounding practical applicability and scalability are key concerns that should be addressed. These improvements would make the submission stronger for consideration at a top-tier venue.