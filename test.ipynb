{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openreview\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openreview_credentials():\n",
    "    # First check if the credentials are in the .openreview_credentials file\n",
    "    credentials_file = \".openreview_credentials\"\n",
    "    if os.path.exists(credentials_file):\n",
    "        with open(credentials_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            username = lines[0].strip()\n",
    "            password = lines[1].strip()\n",
    "            return username, password\n",
    "\n",
    "    # If not found, prompt the user to enter the credentials\n",
    "    username = input(\"Enter your OpenReview username: \")\n",
    "    password = input(\"Enter your OpenReview password: \")\n",
    "    return username, password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "username, password = get_openreview_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openreview.api.OpenReviewClient(\n",
    "    baseurl='https://api2.openreview.net',\n",
    "    username=username,\n",
    "    password=password\n",
    ")\n",
    "venue_id = 'ICLR.cc/2024/Conference'\n",
    "venue_group = client.get_group(venue_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting V2 Notes: 100%|█████████▉| 9118/9128 [00:02<00:00, 3165.30it/s]\n"
     ]
    }
   ],
   "source": [
    "submission_name = venue_group.content['submission_name']['value']\n",
    "invitation_name = f'{venue_id}/-/{submission_name}'\n",
    "submissions = client.get_all_notes(invitation=invitation_name)\n",
    "submissions_with_replies = client.get_notes(invitation=invitation_name, details='replies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = submissions_with_replies[0]\n",
    "# replies = [r for r in s.details['replies'] if r['invitations'][0].endswith('Official_Comment')]\n",
    "replies = [r for r in s.details['replies']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in submissions_with_replies:\n",
    "  if len(s.details['replies']) > 15:\n",
    "    submission_with_replies = s\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 replies for the submission rhgIgTSSxW\n"
     ]
    }
   ],
   "source": [
    "replies = s.details['replies']\n",
    "print(f\"There are {len(replies)} replies for the submission {s.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_key_value_pairs(d, indent=0):\n",
    "    for k, v in d.items():\n",
    "        # if isinstance(v, dict) and 'value' in v.keys():\n",
    "        #     v = v['value']\n",
    "        if isinstance(v, dict):\n",
    "            print(\"  \" * indent, k)\n",
    "            print_key_value_pairs(v, indent + 1)\n",
    "        else:\n",
    "            print(\"  \" * indent, k, \":\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reply_source_type(reply):\n",
    "    signature = reply['signatures'][0]\n",
    "    # Parse the trailing value after the last \"/\" in the signature\n",
    "    id = signature.split('/')[-1]\n",
    "    if 'Reviewer' in signature:\n",
    "        return 'reviewer', id\n",
    "    elif 'Authors' in signature:\n",
    "        return 'author', id\n",
    "    elif 'Area_Chair' in signature:\n",
    "        return 'area_chair', id\n",
    "    elif 'Program_Chair' in signature:\n",
    "        return 'program_chair', id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Reply from reviewer Reviewer_EnVq\n",
      "1: Reply from reviewer Reviewer_Ly5o\n",
      "2: Reply from reviewer Reviewer_fJsx\n",
      "3: Reply from reviewer Reviewer_aaV3\n",
      "4: Reply from author \n",
      "5: Reply from author \n",
      "6: Reply from author \n",
      "7: Reply from author \n",
      "8: Reply from author \n",
      "9: Reply from author \n",
      "10: Reply from author \n",
      "11: Reply from author \n",
      "12: Reply from author \n",
      "13: Reply from author \n",
      "14: Reply from author \n",
      "15: Reply from author \n",
      "16: Reply from reviewer Reviewer_aaV3\n",
      "17: Reply from author \n",
      "18: Reply from author \n",
      "19: Reply from reviewer Reviewer_EnVq\n",
      "20: Reply from area_chair Area_Chair_6yPy\n",
      "21: Reply from program_chair Program_Chairs\n"
     ]
    }
   ],
   "source": [
    "for i, reply in enumerate(replies):\n",
    "    reply_type, reply_id = get_reply_source_type(reply)\n",
    "    print(f\"{i}: Reply from {reply_type} {reply_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'summary': {'value': 'This paper considers the problem of making predictions on tabular data. The authors propose a retrieval-augmented approach where a predictor takes the representation not of the table being predicted but also the representation of the nearest neighbors from a training dataset. The encoding representations and the predictors are training together and use straightforward architecture architectures. The main result is that a combination of the carefully crafted techniques outperforms GBDT on an ensemble of tasks. The training time is higher than GBDT but not unreasonable, and better compared to prior deep learning methods. The prediction times are better'},\n",
       "  'soundness': {'value': '3 good'},\n",
       "  'presentation': {'value': '3 good'},\n",
       "  'contribution': {'value': '3 good'},\n",
       "  'strengths': {'value': '1. The results seem to be a significant advance over prior work in tabular data predictions. In particular, the first deep learning model to outperform GBDT on an ensemble of datasets.\\n2. The experiments and analysis are quite extensive. Multiple datasets of different kinds of data, analysis of training and prediction times.\\n3. Clear articulation of which techniques helped. the techniques are overall not too complex.'},\n",
       "  'weaknesses': {'value': 'A comparison of the inference and query complexity between the methods is lacking.'},\n",
       "  'questions': {'value': '1. Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more? How does the inference complexity depend on the size of the table data?\\n\\n2. Could a different selection of datasets prove that the tabR is not superior to GBDT? In other words, are these datasets highly representative?\\n\\n3. Is it not surprising that Step-1 (adding context labels) did not help that much? One would guess that this is a big component of signal in retrieval augmentation.\\n\\n4. Not a question, but the methodology here reminds one of extreme classification and specifically this paper. https://arxiv.org/abs/2207.04452'},\n",
       "  'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "  'rating': {'value': '8: accept, good paper'},\n",
       "  'confidence': {'value': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'},\n",
       "  'code_of_conduct': {'value': 'Yes'}},\n",
       " 'id': '7UJsyfPyd4',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': 'rhgIgTSSxW',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3'],\n",
       " 'nonreaders': [],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3'],\n",
       " 'number': 4,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Official_Review',\n",
       "  'ICLR.cc/2024/Conference/-/Edit'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1698809806217,\n",
       " 'cdate': 1698809806217,\n",
       " 'tmdate': 1699637193544,\n",
       " 'mdate': 1699637193544,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'title': {'value': 'Rebuttal (part 1/2)'},\n",
       "  'comment': {'value': 'We thank the reviewer for the positive feedback!\\n\\n> A comparison of the inference and query complexity between the methods is lacking.\\n\\n**In the new PDF, this is addressed in Section A.4.2.** For convenience, here, we provide a summary.\\n\\nBelow, we report the inference throughput of TabR and XGBoost.\\n\\n(The technical setup:\\n- XGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\\n- Computation is performed on NVIDIA 2080 Ti.\\n- For both models, objects are passed by batches of 4096 objects.)\\n\\n**The key observations:**\\n- On the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\\n- **Important**: our implementation of TabR is naive and lacks even basic optimizations.\\n\\n|                                | CH    | CA   | HO   | AD   | DI   | OT   | HI  | BL   | WE   | CO   | MI   |\\n|--------------------------------|------|------|------|------|------|-----|------|------|------|------|------|\\n| `#trainingObjects`                | 6400 | 13209 | 14581 | 26048 | 34521 | 39601 | 62751 | 106764 | 296554 | 371847 | 723412 |\\n| `#features`                      | 11    | 8    | 16   | 14   | 9    | 93  | 28   | 9    |  119 | 54   | 136 |\\n| `n_estimators` in XGBoost         | 121   | 3997 | 1328 | 988  | 802  | 524 | 1040 | 1751 | 3999 | 1258 | 3814 |\\n| `max_depth` in XGBoost            | 5     | 9    | 7    | 10   | 13   | 13  | 11   | 8    | 13   | 12   | 12   |\\n| XGBoost throughput (obj./sec.)  | 2197k | 33k  | 179k | 131k | 417k | 19k | 72k  | 84k  | 15k  | 10k  | 14k  |\\n| TabR-S throughput (obj./sec.)  | 35k   | 35k  | 55k  | 33k  | 43k  | 40k | 37k  | 27k  | 34k  | 23k  | 11k  |\\n| Overhead                        | 62.3  | 0.9  | 3.3  | 3.9  | 9.6  | 0.5 | 1.9  | 3.1  | 0.5  | 0.4  | 1.2  |\\n\\n**In more detail:**\\n- On \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\\n- On \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\\n- With the growth of training size (MI), TabR may become slower because of the retrieval, however, there is *a lot* of room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\\n\\n> Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more?\\n\\nStrictly speaking, the results reported in all main tables are obtained without fixing the inference time budget: all methods can spend any time they need to make predictions. *However,* generally, the efficiency of TabR vs. prior work is an important storyline of our paper with significant wins over prior work, and, in the considered scope of datasets, the inference time of TabR is always reasonable (as indicated by the above table).\\n\\n> How does the inference complexity depend on the size of the table data?\\n\\nAs indicated by the above table, for datasets of sizes up to (roughly) 500K objects, for TabR-S, the dataset size is not a major factor defining the inference throughput. For larger datasets, the dataset size becomes a more important factor (formally, for the current brute force search, the search complexity grows linearly with the dataset size). Luckily, the current implementation of TabR has a lot of room for simple optimizations:\\n- caching candidate key representations instead of recomputing them on each forward pass.\\n- performing the search in float16 instead of the current float32.\\n- performing approximate similarity search instead of the current brute force.\\n- using only a subset of the training data as candidates.\\n- etc.'}},\n",
       " 'id': 'pfSPP72xj0',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': '7UJsyfPyd4',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Submission9502/Authors'],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Authors'],\n",
       " 'number': 1,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Official_Comment'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1700325638178,\n",
       " 'cdate': 1700325638178,\n",
       " 'tmdate': 1700325638178,\n",
       " 'mdate': 1700325638178,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'title': {'value': 'Paper Decision'},\n",
       "  'decision': {'value': 'Accept (poster)'},\n",
       "  'comment': {'value': ''}},\n",
       " 'id': 'wd7zIBoZIw',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': 'rhgIgTSSxW',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Program_Chairs'],\n",
       " 'nonreaders': [],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Program_Chairs'],\n",
       " 'number': 1,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Decision',\n",
       "  'ICLR.cc/2024/Conference/-/Edit'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1705405993702,\n",
       " 'cdate': 1705405993702,\n",
       " 'tmdate': 1708116196470,\n",
       " 'mdate': 1708116196470,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'metareview': {'value': 'This submission contributes a neural architecture dedicated to tabular learning based on combining a feed-forward network with a nearest neighbor mechanism. The submission generated many solid discussions and was seen as an interesting addition to the tabular-learning literature. The reviewers appreciated the extensive experiments, the writing, and the reproducibility of the work. More baselines could have been added, and more attention to categorical variables.'},\n",
       "  'justification_for_why_not_higher_score': {'value': 'The paper is already borderline with regards to acceptance. I do not think that we can push it further.'},\n",
       "  'justification_for_why_not_lower_score': {'value': 'The work seems solid, as acknowledged by 3 of the 4 reviewers. The answers to the review by the authors were also solid. The fourth reviewer seems unfair, and is the author of one of the competing methods puts forward in the critical review. The work seems to honestly position itself relative to this prior art.'}},\n",
       " 'id': 'aqqcyUzuOm',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': 'rhgIgTSSxW',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Submission9502/Area_Chair_6yPy'],\n",
       " 'nonreaders': [],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Senior_Area_Chairs',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Area_Chair_6yPy'],\n",
       " 'number': 1,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Meta_Review',\n",
       "  'ICLR.cc/2024/Conference/-/Edit'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1702050837757,\n",
       " 'cdate': 1702050837757,\n",
       " 'tmdate': 1708115502768,\n",
       " 'mdate': 1708115502768,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'title': {'value': 'inference time comparison'},\n",
       "  'comment': {'value': 'Is it reasonable to compare XGBoost with deep learning with 4096 batch size on GPU. How do the inference times compare on CPU or with small batching as is the case with inference in reality?'}},\n",
       " 'id': '6bGJyzEwju',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': 'pfSPP72xj0',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3'],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Reviewer_aaV3'],\n",
       " 'number': 13,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Official_Comment'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1700517720978,\n",
       " 'cdate': 1700517720978,\n",
       " 'tmdate': 1700517720978,\n",
       " 'mdate': 1700517720978,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00: review from reviewer Reviewer_EnVq\n",
      "01: review from reviewer Reviewer_Ly5o\n",
      "02: review from reviewer Reviewer_fJsx\n",
      "03: review from reviewer Reviewer_aaV3\n",
      "04: author comment from author \n",
      "05: author comment from author \n",
      "06: author comment from author \n",
      "07: author comment from author \n",
      "08: author comment from author \n",
      "09: author comment from author \n",
      "10: author comment from author \n",
      "11: author comment from author \n",
      "12: author comment from author \n",
      "13: author comment from author \n",
      "14: author comment from author \n",
      "15: author comment from author \n",
      "16: reviewer comment from reviewer Reviewer_aaV3\n",
      "17: author comment from author \n",
      "18: author comment from author \n",
      "19: reviewer comment from reviewer Reviewer_EnVq\n",
      "20: meta_review from area_chair Area_Chair_6yPy\n",
      "21: decision from program_chair Program_Chairs\n"
     ]
    }
   ],
   "source": [
    "def get_reply_type(reply):\n",
    "    def any_part_matches(s, parts):\n",
    "        return any([s in p for p in parts])\n",
    "    source_type, _ = get_reply_source_type(reply)\n",
    "    if any_part_matches('Official_Comment', reply['invitations']):\n",
    "        return source_type + ' comment'\n",
    "    elif any_part_matches('Meta_Review', reply['invitations']):\n",
    "        return 'meta_review'\n",
    "    elif any_part_matches('Review', reply['invitations']):\n",
    "        return 'review'\n",
    "    elif any_part_matches('Decision', reply['invitations']):\n",
    "        return 'decision'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "for i, reply in enumerate(replies):\n",
    "    reply_type = get_reply_type(reply)\n",
    "    source_type, source_id = get_reply_source_type(reply)\n",
    "    print(f\"{i:02d}: {reply_type} from {source_type} {source_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ICLR.cc/2024/Conference/Submission9502/-/Decision',\n",
       " 'ICLR.cc/2024/Conference/-/Edit']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply['invitations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_openreview_review(review_obj):\n",
    "    \"\"\"\n",
    "    Converts an OpenReview review dictionary into a human-readable string\n",
    "    with labeled sections.\n",
    "    \n",
    "    :param review_obj: A dictionary representing the review object from OpenReview.\n",
    "    :return: A formatted string summarizing the review.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract top-level fields:\n",
    "    review_id = review_obj.get(\"id\", \"N/A\")\n",
    "    forum_id = review_obj.get(\"forum\", \"N/A\")\n",
    "    signatures = review_obj.get(\"signatures\", [])\n",
    "    signature_str = \", \".join(signatures) if signatures else \"N/A\"\n",
    "\n",
    "    # Extract content fields (the actual review text sections):\n",
    "    content = review_obj.get(\"content\", {})\n",
    "\n",
    "    def fetch_value(field_key):\n",
    "        \"\"\"\n",
    "        Safely fetch the text under content[field_key]['value'], \n",
    "        returning a placeholder if missing or empty.\n",
    "        \"\"\"\n",
    "        return content.get(field_key, {}).get(\"value\", \"[No information provided]\")\n",
    "\n",
    "    # Commonly found review sections in OpenReview:\n",
    "    rating         = fetch_value(\"rating\")\n",
    "    confidence     = fetch_value(\"confidence\")\n",
    "    summary        = fetch_value(\"summary\")\n",
    "    soundness      = fetch_value(\"soundness\")\n",
    "    presentation   = fetch_value(\"presentation\")\n",
    "    contribution   = fetch_value(\"contribution\")\n",
    "    strengths      = fetch_value(\"strengths\")\n",
    "    weaknesses     = fetch_value(\"weaknesses\")\n",
    "    questions      = fetch_value(\"questions\")\n",
    "\n",
    "    # Build a readable string with labeled sections:\n",
    "    # Adjust formatting as needed (e.g., add bullet points, etc.).\n",
    "    lines = [\n",
    "        f\"Review ID: {review_id}\",\n",
    "        # f\"Forum (Submission) ID: {forum_id}\",\n",
    "        # f\"Review by: {signature_str}\",\n",
    "        # \"\",\n",
    "        \"\",\n",
    "        \"=== Summary ===\",\n",
    "        summary,\n",
    "        \"\",\n",
    "        \"=== Soundness ===\",\n",
    "        soundness,\n",
    "        \"\",\n",
    "        \"=== Presentation ===\",\n",
    "        presentation,\n",
    "        \"\",\n",
    "        \"=== Contribution ===\",\n",
    "        contribution,\n",
    "        \"\",\n",
    "        \"=== Strengths ===\",\n",
    "        strengths,\n",
    "        \"\",\n",
    "        \"=== Weaknesses ===\",\n",
    "        weaknesses,\n",
    "        \"\",\n",
    "        \"=== Questions ===\",\n",
    "        questions,\n",
    "        f\"Rating: {rating}\",\n",
    "        f\"Confidence: {confidence}\",\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review ID: yVdQ7kKCcl\n",
      "\n",
      "=== Summary ===\n",
      "The authors meticulously designed a supervised deep learning model for tabular data prediction, which operates in a retrieval-like manner. It outperformed tree-based models on middle-scale datasets, as well as other retrieval-based deep learning tabular learning models. To achieve this, they introduced a k-Nearest-Neighbors-like idea in model design.\n",
      "\n",
      "=== Soundness ===\n",
      "3 good\n",
      "\n",
      "=== Presentation ===\n",
      "2 fair\n",
      "\n",
      "=== Contribution ===\n",
      "2 fair\n",
      "\n",
      "=== Strengths ===\n",
      "- As emphasized by the authors, their method has managed to outperform tree based models like xgboost on middle-scale datasets.\n",
      "\n",
      "- Overall, the presentation is clear, and the experiments are comprehensive. The details are clear and the model is highly reproducible.\n",
      "\n",
      "- This model is the best-performing retrieval based model.\n",
      "\n",
      "=== Weaknesses ===\n",
      "- The motivations behind the module designs are not entirely clear. It appears that the authors made meticulous module (equation) optimization based on its performance on some datasets empirically. Then: \n",
      "\n",
      "(1) Why does employing the L2 distance, instand of the dot product, lead to improved performance (as shown in Eq. 3)? \n",
      "\n",
      "(2) Why is the T function required to use LinearWithoutBias? \n",
      "\n",
      "(3) We are uncertain about the robustness of the designed modules. If the dataset characteristics are changed, is it likely that the performance rankings will change significantly? The performances only on middle-sized datasets cannot show the robustness.\n",
      "\n",
      "...\n",
      "\n",
      "I suggest that providing a theoretical analysis or intuitive motivation would enhance the reader's understanding of those details.\n",
      "\n",
      "- Some sota DL approaches are not compared, such as T2G-Former (an improved version of FTT)[1], TabPFN [2], and TANGOS [3]. Especially, TabFPN is relatively similar to TabR. These papers are current SOTA, and may outperforms tree based models.\n",
      "\n",
      "[1] T2G-Former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction\n",
      "\n",
      "[2] TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n",
      "\n",
      "[3] TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization\n",
      "\n",
      "- The major comparison lies among middle-scale datasets, accompanied with some results on few other datasets shown in Table 3. In scenarios involving sparse, medium, and dense data-distributed datasets (which typically occur in small, medium-sized, and large-sized datasets, respectively), I suppose that there exists a variance in the nearest neighbor retrieval pattern. Hence, conducting tests solely on medium-sized datasets may not suffice. Furthermore, the issue of inefficiency when dealing with large-scaled datasets appears to have hindered the authors from proving the method's effectiveness in large-scaled datasets.\n",
      "\n",
      "- The method proposed by the authors appears to have achieved slight performance advantages on certain datasets (although some SOTA are not compared). However, due to the lacks of explanation for the model details that are designed empirically, it seems unnecessary and risky to apply this method in real-world scenarios (for example, it's unclear whether L2 distance may fail when uninformative features are present; or, for instance, when a table has a feature with values [f_1, f_2, f_3, ..., f_n], and we take the logarithm of these values [log f_1, log f_2, log f_3, ..., log f_n] or their reciprocals, the method may perform poorly in such cases).\n",
      "\n",
      "=== Questions ===\n",
      "- In Section 3.1, you mentioned \"continuous (i.e., continuous) features.\" Could this be a typographical error?\n",
      "\n",
      "- I am curious if the L2 design is sensitive to uninformative features? You can offer some analysis or conduct experiments by adding some gaussian noise columns (uninformative features are commonly seen in tabular datasets) and observe the change of performances. Some transformation like logarithm may impact the results.\n",
      "\n",
      "- Some questions in weakness.\n",
      "Rating: 3: reject, not good enough\n",
      "Confidence: 4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.\n"
     ]
    }
   ],
   "source": [
    "print(parse_openreview_review(replies[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'title': {'value': 'Rebuttal (part 1/2)'},\n",
       "  'comment': {'value': 'We thank the reviewer for the positive feedback!\\n\\n> A comparison of the inference and query complexity between the methods is lacking.\\n\\n**In the new PDF, this is addressed in Section A.4.2.** For convenience, here, we provide a summary.\\n\\nBelow, we report the inference throughput of TabR and XGBoost.\\n\\n(The technical setup:\\n- XGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\\n- Computation is performed on NVIDIA 2080 Ti.\\n- For both models, objects are passed by batches of 4096 objects.)\\n\\n**The key observations:**\\n- On the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\\n- **Important**: our implementation of TabR is naive and lacks even basic optimizations.\\n\\n|                                | CH    | CA   | HO   | AD   | DI   | OT   | HI  | BL   | WE   | CO   | MI   |\\n|--------------------------------|------|------|------|------|------|-----|------|------|------|------|------|\\n| `#trainingObjects`                | 6400 | 13209 | 14581 | 26048 | 34521 | 39601 | 62751 | 106764 | 296554 | 371847 | 723412 |\\n| `#features`                      | 11    | 8    | 16   | 14   | 9    | 93  | 28   | 9    |  119 | 54   | 136 |\\n| `n_estimators` in XGBoost         | 121   | 3997 | 1328 | 988  | 802  | 524 | 1040 | 1751 | 3999 | 1258 | 3814 |\\n| `max_depth` in XGBoost            | 5     | 9    | 7    | 10   | 13   | 13  | 11   | 8    | 13   | 12   | 12   |\\n| XGBoost throughput (obj./sec.)  | 2197k | 33k  | 179k | 131k | 417k | 19k | 72k  | 84k  | 15k  | 10k  | 14k  |\\n| TabR-S throughput (obj./sec.)  | 35k   | 35k  | 55k  | 33k  | 43k  | 40k | 37k  | 27k  | 34k  | 23k  | 11k  |\\n| Overhead                        | 62.3  | 0.9  | 3.3  | 3.9  | 9.6  | 0.5 | 1.9  | 3.1  | 0.5  | 0.4  | 1.2  |\\n\\n**In more detail:**\\n- On \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\\n- On \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\\n- With the growth of training size (MI), TabR may become slower because of the retrieval, however, there is *a lot* of room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\\n\\n> Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more?\\n\\nStrictly speaking, the results reported in all main tables are obtained without fixing the inference time budget: all methods can spend any time they need to make predictions. *However,* generally, the efficiency of TabR vs. prior work is an important storyline of our paper with significant wins over prior work, and, in the considered scope of datasets, the inference time of TabR is always reasonable (as indicated by the above table).\\n\\n> How does the inference complexity depend on the size of the table data?\\n\\nAs indicated by the above table, for datasets of sizes up to (roughly) 500K objects, for TabR-S, the dataset size is not a major factor defining the inference throughput. For larger datasets, the dataset size becomes a more important factor (formally, for the current brute force search, the search complexity grows linearly with the dataset size). Luckily, the current implementation of TabR has a lot of room for simple optimizations:\\n- caching candidate key representations instead of recomputing them on each forward pass.\\n- performing the search in float16 instead of the current float32.\\n- performing approximate similarity search instead of the current brute force.\\n- using only a subset of the training data as candidates.\\n- etc.'}},\n",
       " 'id': 'pfSPP72xj0',\n",
       " 'forum': 'rhgIgTSSxW',\n",
       " 'replyto': '7UJsyfPyd4',\n",
       " 'signatures': ['ICLR.cc/2024/Conference/Submission9502/Authors'],\n",
       " 'readers': ['everyone'],\n",
       " 'writers': ['ICLR.cc/2024/Conference',\n",
       "  'ICLR.cc/2024/Conference/Submission9502/Authors'],\n",
       " 'number': 1,\n",
       " 'invitations': ['ICLR.cc/2024/Conference/Submission9502/-/Official_Comment'],\n",
       " 'domain': 'ICLR.cc/2024/Conference',\n",
       " 'tcdate': 1700325638178,\n",
       " 'cdate': 1700325638178,\n",
       " 'tmdate': 1700325638178,\n",
       " 'mdate': 1700325638178,\n",
       " 'license': 'CC BY 4.0',\n",
       " 'version': 2}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuttal (part 1/2)\n",
      "We thank the reviewer for the positive feedback!\n",
      "\n",
      "> A comparison of the inference and query complexity between the methods is lacking.\n",
      "\n",
      "**In the new PDF, this is addressed in Section A.4.2.** For convenience, here, we provide a summary.\n",
      "\n",
      "Below, we report the inference throughput of TabR and XGBoost.\n",
      "\n",
      "(The technical setup:\n",
      "- XGBoost and TabR-S with tuned hyperparameters as in Table 4 of the main text\n",
      "- Computation is performed on NVIDIA 2080 Ti.\n",
      "- For both models, objects are passed by batches of 4096 objects.)\n",
      "\n",
      "**The key observations:**\n",
      "- On the considered datasets, the throughputs of TabR and XGBoost are mostly comparable.\n",
      "- **Important**: our implementation of TabR is naive and lacks even basic optimizations.\n",
      "\n",
      "|                                | CH    | CA   | HO   | AD   | DI   | OT   | HI  | BL   | WE   | CO   | MI   |\n",
      "|--------------------------------|------|------|------|------|------|-----|------|------|------|------|------|\n",
      "| `#trainingObjects`                | 6400 | 13209 | 14581 | 26048 | 34521 | 39601 | 62751 | 106764 | 296554 | 371847 | 723412 |\n",
      "| `#features`                      | 11    | 8    | 16   | 14   | 9    | 93  | 28   | 9    |  119 | 54   | 136 |\n",
      "| `n_estimators` in XGBoost         | 121   | 3997 | 1328 | 988  | 802  | 524 | 1040 | 1751 | 3999 | 1258 | 3814 |\n",
      "| `max_depth` in XGBoost            | 5     | 9    | 7    | 10   | 13   | 13  | 11   | 8    | 13   | 12   | 12   |\n",
      "| XGBoost throughput (obj./sec.)  | 2197k | 33k  | 179k | 131k | 417k | 19k | 72k  | 84k  | 15k  | 10k  | 14k  |\n",
      "| TabR-S throughput (obj./sec.)  | 35k   | 35k  | 55k  | 33k  | 43k  | 40k | 37k  | 27k  | 34k  | 23k  | 11k  |\n",
      "| Overhead                        | 62.3  | 0.9  | 3.3  | 3.9  | 9.6  | 0.5 | 1.9  | 3.1  | 0.5  | 0.4  | 1.2  |\n",
      "\n",
      "**In more detail:**\n",
      "- On \"non-simple\" tasks (CA, OT, WE, CO), TabR is faster (\"non-simple\" = XGBoost needs many trees AND/OR XGBoost needs high depth AND/OR a dataset has more features).\n",
      "- On \"simple\" tasks, XGBoost is faster (\"simple\" = XGBoost is shallow AND/OR dataset has few features).\n",
      "- With the growth of training size (MI), TabR may become slower because of the retrieval, however, there is *a lot* of room for optimizations (caching candidate representations instead of recomputing them on each forward pass; using float16 instead of the current float32; doing approximate search instead of the current brute force; using only a subset of the training data as candidates, etc.)\n",
      "\n",
      "> Inference time and compexity -- are the studies based on normalized inference time between models? If not, could you comment more?\n",
      "\n",
      "Strictly speaking, the results reported in all main tables are obtained without fixing the inference time budget: all methods can spend any time they need to make predictions. *However,* generally, the efficiency of TabR vs. prior work is an important storyline of our paper with significant wins over prior work, and, in the considered scope of datasets, the inference time of TabR is always reasonable (as indicated by the above table).\n",
      "\n",
      "> How does the inference complexity depend on the size of the table data?\n",
      "\n",
      "As indicated by the above table, for datasets of sizes up to (roughly) 500K objects, for TabR-S, the dataset size is not a major factor defining the inference throughput. For larger datasets, the dataset size becomes a more important factor (formally, for the current brute force search, the search complexity grows linearly with the dataset size). Luckily, the current implementation of TabR has a lot of room for simple optimizations:\n",
      "- caching candidate key representations instead of recomputing them on each forward pass.\n",
      "- performing the search in float16 instead of the current float32.\n",
      "- performing approximate similarity search instead of the current brute force.\n",
      "- using only a subset of the training data as candidates.\n",
      "- etc.\n"
     ]
    }
   ],
   "source": [
    "print(replies[4]['content']['title']['value'])\n",
    "print(replies[4]['content']['comment']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
